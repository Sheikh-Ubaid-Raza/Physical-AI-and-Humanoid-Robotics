"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[5494],{3881:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"week-01/sensor-systems","title":"Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors","description":"Learning Objectives","source":"@site/docs/week-01/sensor-systems.md","sourceDirName":"week-01","slug":"/week-01/sensor-systems","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-01/sensor-systems","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-01/sensor-systems.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors"},"sidebar":"tutorialSidebar","previous":{"title":"Foundations of Physical AI and Embodied Intelligence","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-01/foundations-of-physical-ai"},"next":{"title":"Embodied Intelligence: Learning from Body-Environment Interactions","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-02/embodied-intelligence"}}');var t=s(4848),a=s(8453);const r={sidebar_position:2,title:"Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors"},o="Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"LIDAR Sensors",id:"lidar-sensors",level:3},{value:"Camera Systems",id:"camera-systems",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:3},{value:"Force/Torque Sensors",id:"forcetorque-sensors",level:3},{value:"Code Example 1: LIDAR Data Processing",id:"code-example-1-lidar-data-processing",level:2},{value:"Code Example 2: IMU Data Integration",id:"code-example-2-imu-data-integration",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"sensor-systems-lidar-cameras-imus-forcetorque-sensors",children:"Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Describe different types of sensors used in robotics"}),"\n",(0,t.jsx)(n.li,{children:"Explain how LIDAR, cameras, IMUs, and force/torque sensors work"}),"\n",(0,t.jsx)(n.li,{children:"Implement basic sensor data processing algorithms"}),"\n",(0,t.jsx)(n.li,{children:"Compare advantages and disadvantages of different sensor types"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of physical quantities (distance, angles, force, acceleration)"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with coordinate systems and transformations"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,t.jsx)(n.p,{children:"Robotic systems rely heavily on sensor data to perceive and interact with their environment. Different sensor modalities provide complementary information that enables robots to navigate, manipulate objects, and safely interact with humans."}),"\n",(0,t.jsx)(n.h3,{id:"lidar-sensors",children:"LIDAR Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Light Detection and Ranging (LIDAR) sensors emit laser pulses and measure the time it takes for the light to return after reflecting off surfaces. This provides accurate distance measurements to surrounding objects. LIDARs are excellent for creating 2D or 3D maps of the environment and detecting obstacles."}),"\n",(0,t.jsx)(n.h3,{id:"camera-systems",children:"Camera Systems"}),"\n",(0,t.jsx)(n.p,{children:"Cameras capture visual information from the environment. RGB cameras provide color information, while stereo cameras can estimate depth. Modern computer vision algorithms can extract rich semantic information from camera images, including object detection, recognition, and scene understanding."}),"\n",(0,t.jsx)(n.h3,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,t.jsx)(n.p,{children:"IMUs combine accelerometers, gyroscopes, and sometimes magnetometers to measure the robot's motion and orientation. They provide high-frequency measurements of acceleration and angular velocity, which are crucial for robot stabilization and motion control."}),"\n",(0,t.jsx)(n.h3,{id:"forcetorque-sensors",children:"Force/Torque Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Force/torque sensors measure the forces and torques applied to the robot. They are essential for precise manipulation tasks, enabling robots to apply appropriate forces when grasping objects or interacting with the environment."}),"\n",(0,t.jsx)(n.h2,{id:"code-example-1-lidar-data-processing",children:"Code Example 1: LIDAR Data Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Purpose: Process LIDAR data to detect obstacles and create occupancy grid\n# Setup Instructions: Install numpy\n# Run: python lidar_processing.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef process_lidar_scan(ranges, angle_min, angle_max):\n    """Process LIDAR scan data to extract obstacle information"""\n    # Calculate angle for each measurement\n    angle_increment = (angle_max - angle_min) / len(ranges)\n    angles = [angle_min + i * angle_increment for i in range(len(ranges))]\n\n    # Extract obstacle positions in Cartesian coordinates\n    obstacle_points = []\n    for i, r in enumerate(ranges):\n        if 0.1 < r < 10.0:  # Valid range\n            angle = angles[i]\n            x = r * np.cos(angle)\n            y = r * np.sin(angle)\n            obstacle_points.append([x, y])\n\n    return np.array(obstacle_points)\n\n# Example usage\nsample_ranges = [float(\'inf\')] * 10 + [2.5, 2.4, 2.3, 2.2, 2.1] + [2.0] * 10 + [1.8, 1.5, 1.2, 0.8, 0.5] + [float(\'inf\')] * 10\nangles_min = -np.pi/2\nangles_max = np.pi/2\n\nobstacle_points = process_lidar_scan(sample_ranges, angles_min, angles_max)\nprint(f"Detected {len(obstacle_points)} obstacle points")\nif len(obstacle_points) > 0:\n    print(f"First few points: {obstacle_points[:3]}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Detected 10 obstacle points\nFirst few points: [[ 0.          2.5       ]\n [ 0.23776413  2.48859624]\n [ 0.46947156  2.45484353]]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"code-example-2-imu-data-integration",children:"Code Example 2: IMU Data Integration"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Purpose: Integrate IMU data to estimate position and orientation\n# Setup Instructions: Install numpy\n# Run: python imu_integration.py\n\nimport numpy as np\n\nclass IMUPoseEstimator:\n    def __init__(self):\n        self.position = np.zeros(3)  # x, y, z\n        self.velocity = np.zeros(3)  # vx, vy, vz\n        self.orientation = np.array([0, 0, 0, 1])  # quaternion [x, y, z, w]\n\n    def update(self, linear_acceleration, angular_velocity, dt):\n        """Update pose estimate using IMU measurements"""\n        # Integrate acceleration to get velocity\n        self.velocity += linear_acceleration * dt\n\n        # Integrate velocity to get position\n        self.position += self.velocity * dt\n\n        # Update orientation using angular velocity\n        # This is a simplified integration for demonstration\n        dq = self._angular_velocity_to_quaternion_derivative(\n            self.orientation, angular_velocity\n        )\n        self.orientation += dq * dt\n        # Normalize quaternion\n        self.orientation /= np.linalg.norm(self.orientation)\n\n    def _angular_velocity_to_quaternion_derivative(self, q, omega):\n        """Convert angular velocity to quaternion derivative"""\n        wx, wy, wz = omega\n        Omega = np.array([\n            [0, -wx, -wy, -wz],\n            [wx, 0, wz, -wy],\n            [wy, -wz, 0, wx],\n            [wz, wy, -wx, 0]\n        ])\n        return 0.5 * Omega @ q\n\n# Example usage\nestimator = IMUPoseEstimator()\nlinear_acc = np.array([0.1, 0.05, 9.81])  # Including gravity\nangular_vel = np.array([0.1, 0.05, 0.02])\ndt = 0.01  # 10ms time step\n\nprint(f"Initial position: {estimator.position}")\nestimator.update(linear_acc, angular_vel, dt)\nprint(f"After 10ms: position={estimator.position}, velocity={estimator.velocity}")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Initial position: [0. 0. 0.]\nAfter 10ms: position=[0.000005   0.0000025  0.0981049 ], velocity=[0.001  0.0005 0.981 ]\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a function that fuses data from multiple sensors (LIDAR, camera, IMU) to create a more robust perception system"}),"\n",(0,t.jsx)(n.li,{children:"Create a simple Kalman filter to smooth noisy sensor measurements"}),"\n",(0,t.jsx)(n.li,{children:"Implement a basic SLAM (Simultaneous Localization and Mapping) algorithm using sensor data"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Sensor systems form the foundation of robotic perception. Understanding how different sensors work and how to process their data is crucial for building effective robotic systems. The choice of sensors depends on the specific application requirements, environmental conditions, and accuracy needs."}),"\n",(0,t.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,t.jsx)(n.p,{children:"This chapter can be completed using simulation environments. For physical implementation, the following hardware is recommended:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"2D LIDAR (e.g., RPLIDAR A1/A2)"}),"\n",(0,t.jsx)(n.li,{children:"RGB-D camera (e.g., Intel Realsense D435)"}),"\n",(0,t.jsx)(n.li,{children:"IMU (e.g., Bosch BNO055)"}),"\n",(0,t.jsx)(n.li,{children:"Force/torque sensor (e.g., ATI Nano 25)"}),"\n",(0,t.jsx)(n.li,{children:"Robot platform with sensor interfaces"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>o});var i=s(6540);const t={},a=i.createContext(t);function r(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);