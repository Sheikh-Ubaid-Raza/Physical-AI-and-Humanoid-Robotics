"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[4393],{2362:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"week-01/foundations-of-physical-ai","title":"Foundations of Physical AI and Embodied Intelligence","description":"Learning Objectives","source":"@site/docs/week-01/foundations-of-physical-ai.md","sourceDirName":"week-01","slug":"/week-01/foundations-of-physical-ai","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-01/foundations-of-physical-ai","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-01/foundations-of-physical-ai.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Foundations of Physical AI and Embodied Intelligence"},"sidebar":"tutorialSidebar","next":{"title":"Sensor Systems: LIDAR, Cameras, IMUs, Force/Torque Sensors","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-01/sensor-systems"}}');var t=i(4848),a=i(8453);const o={sidebar_position:1,title:"Foundations of Physical AI and Embodied Intelligence"},l="Foundations of Physical AI and Embodied Intelligence",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Code Example 1: Basic Physical Simulation",id:"code-example-1-basic-physical-simulation",level:2},{value:"Code Example 2: Sensor Data Processing",id:"code-example-2-sensor-data-processing",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"foundations-of-physical-ai-and-embodied-intelligence",children:"Foundations of Physical AI and Embodied Intelligence"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Define Physical AI and embodied intelligence"}),"\n",(0,t.jsx)(n.li,{children:"Explain the relationship between AI and physical systems"}),"\n",(0,t.jsx)(n.li,{children:"Identify key challenges in Physical AI"}),"\n",(0,t.jsx)(n.li,{children:"Describe applications of Physical AI in robotics"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Basic understanding of AI concepts"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with programming concepts"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,t.jsx)(n.p,{children:"Physical AI represents a paradigm shift from traditional AI that operates primarily in digital spaces to AI systems that interact with and operate in the physical world. Unlike classical AI which processes abstract data, Physical AI must navigate the complexities of real-world physics, uncertainty, and embodied interaction."}),"\n",(0,t.jsx)(n.p,{children:"The concept of embodied intelligence suggests that intelligence emerges from the interaction between an agent and its environment. This perspective challenges the traditional view of intelligence as purely computational, emphasizing instead the role of the body and environment in shaping cognitive processes."}),"\n",(0,t.jsx)(n.p,{children:"Physical AI systems must handle challenges such as:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Real-time perception and action"}),"\n",(0,t.jsx)(n.li,{children:"Uncertainty in sensing and actuation"}),"\n",(0,t.jsx)(n.li,{children:"Physics-aware decision making"}),"\n",(0,t.jsx)(n.li,{children:"Safe interaction with humans and environment"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Key applications include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Autonomous robots"}),"\n",(0,t.jsx)(n.li,{children:"Industrial automation"}),"\n",(0,t.jsx)(n.li,{children:"Healthcare robotics"}),"\n",(0,t.jsx)(n.li,{children:"Service robots"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"code-example-1-basic-physical-simulation",children:"Code Example 1: Basic Physical Simulation"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Purpose: Demonstrates basic physics simulation for a mobile robot\n# Setup Instructions: Install numpy and matplotlib\n# Run: python basic_simulation.py\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass MobileRobot:\n    def __init__(self, x=0, y=0, theta=0):\n        self.x = x  # x position\n        self.y = y  # y position\n        self.theta = theta  # orientation\n\n    def move(self, v, omega, dt):\n        """Move the robot with linear velocity v and angular velocity omega"""\n        self.x += v * np.cos(self.theta) * dt\n        self.y += v * np.sin(self.theta) * dt\n        self.theta += omega * dt\n\n# Example usage\nrobot = MobileRobot()\nprint(f"Initial position: ({robot.x}, {robot.y}, {robot.theta})")\n\n# Move the robot forward\nrobot.move(v=1.0, omega=0.1, dt=0.1)\nprint(f"New position: ({robot.x:.2f}, {robot.y:.2f}, {robot.theta:.2f})")\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Initial position: (0, 0, 0)\nNew position: (0.10, 0.00, 0.01)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"code-example-2-sensor-data-processing",children:"Code Example 2: Sensor Data Processing"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Purpose: Process sensor data from a simulated LIDAR\n# Setup Instructions: Install numpy\n# Run: python sensor_processing.py\n\nimport numpy as np\n\ndef process_lidar_data(raw_data):\n    \"\"\"Process raw LIDAR data to detect obstacles\"\"\"\n    # Convert to numpy array for processing\n    distances = np.array(raw_data)\n\n    # Define obstacle threshold (in meters)\n    obstacle_threshold = 1.0\n\n    # Find indices where obstacles are detected\n    obstacle_indices = np.where(distances < obstacle_threshold)[0]\n\n    # Calculate obstacle positions\n    angle_increment = 2 * np.pi / len(distances)\n    obstacle_angles = obstacle_indices * angle_increment\n\n    return {\n        'obstacle_distances': distances[obstacle_indices],\n        'obstacle_angles': obstacle_angles,\n        'obstacle_count': len(obstacle_indices)\n    }\n\n# Example usage\nlidar_data = [2.5, 1.8, 0.8, 1.2, 3.0, 0.5, 2.1]  # Simulated distances\nobstacles = process_lidar_data(lidar_data)\n\nprint(f\"Detected {obstacles['obstacle_count']} obstacles\")\nfor i, (dist, angle) in enumerate(zip(obstacles['obstacle_distances'], obstacles['obstacle_angles'])):\n    print(f\"Obstacle {i+1}: distance={dist:.2f}m, angle={np.degrees(angle):.2f}\xb0\")\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"Detected 2 obstacles\nObstacle 1: distance=0.80m, angle=102.86\xb0\nObstacle 2: distance=0.50m, angle=261.80\xb0\n"})}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Modify the MobileRobot class to include collision detection with boundaries"}),"\n",(0,t.jsx)(n.li,{children:"Extend the LIDAR processing to identify clusters of obstacles (representing larger objects)"}),"\n",(0,t.jsx)(n.li,{children:"Implement a simple path planning algorithm that avoids detected obstacles"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"Physical AI and embodied intelligence represent a fundamental shift in how we conceptualize artificial intelligence. By grounding AI systems in physical reality, we can create more robust, adaptive, and capable systems that interact meaningfully with the world. This foundation is essential for developing advanced robotic systems that can operate effectively in real-world environments."}),"\n",(0,t.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,t.jsx)(n.p,{children:"This chapter can be completed using simulation environments. For physical implementation, the following hardware is recommended:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Mobile robot platform (e.g., TurtleBot3)"}),"\n",(0,t.jsx)(n.li,{children:"LIDAR sensor (e.g., RPLIDAR A1)"}),"\n",(0,t.jsx)(n.li,{children:"Computer with ROS 2 installed"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);