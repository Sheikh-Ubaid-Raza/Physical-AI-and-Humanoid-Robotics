"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[9900],{8453:(e,n,i)=>{i.d(n,{R:()=>t,x:()=>o});var s=i(6540);const a={},r=s.createContext(a);function t(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:t(e.components),s.createElement(r.Provider,{value:n},e.children)}},9142:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>t,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"week-08/isaac-sdk-setup","title":"Isaac SDK and Isaac Sim Setup","description":"Learning Objectives","source":"@site/docs/week-08/isaac-sdk-setup.md","sourceDirName":"week-08","slug":"/week-08/isaac-sdk-setup","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-08/isaac-sdk-setup","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-08/isaac-sdk-setup.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Isaac SDK and Isaac Sim Setup"},"sidebar":"tutorialSidebar","previous":{"title":"Unity for Robot Visualization","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-07/unity-robot-visualization"},"next":{"title":"AI-Powered Perception and Manipulation","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-09/ai-powered-perception"}}');var a=i(4848),r=i(8453);const t={sidebar_position:1,title:"Isaac SDK and Isaac Sim Setup"},o="Isaac SDK and Isaac Sim Setup",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Key Features of Isaac Sim",id:"key-features-of-isaac-sim",level:3},{value:"Isaac SDK Components",id:"isaac-sdk-components",level:3},{value:"Code Example 1: Isaac Sim Python Extension",id:"code-example-1-isaac-sim-python-extension",level:2},{value:"Code Example 2: Isaac ROS Perception Pipeline",id:"code-example-2-isaac-ros-perception-pipeline",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"isaac-sdk-and-isaac-sim-setup",children:"Isaac SDK and Isaac Sim Setup"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Install and configure the Isaac SDK and Isaac Sim"}),"\n",(0,a.jsx)(n.li,{children:"Create virtual environments for robotics simulation"}),"\n",(0,a.jsx)(n.li,{children:"Import and configure robot models in Isaac Sim"}),"\n",(0,a.jsx)(n.li,{children:"Implement perception and control pipelines using Isaac SDK"}),"\n",(0,a.jsx)(n.li,{children:"Connect Isaac Sim with ROS 2 systems for hybrid simulation"}),"\n",(0,a.jsx)(n.li,{children:"Leverage Isaac Sim's advanced physics and rendering capabilities"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understanding of robotics simulation concepts (covered in Weeks 6-7)"}),"\n",(0,a.jsx)(n.li,{children:"Experience with Docker containers (recommended)"}),"\n",(0,a.jsx)(n.li,{children:"Basic knowledge of CUDA and GPU computing"}),"\n",(0,a.jsx)(n.li,{children:"Familiarity with Python for Isaac SDK development"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim is NVIDIA's comprehensive simulation environment for robotics and AI development. It provides high-fidelity physics simulation, photorealistic rendering, and advanced sensor simulation capabilities. Isaac Sim is built on NVIDIA Omniverse and offers seamless integration with the Isaac SDK for developing perception and control algorithms."}),"\n",(0,a.jsx)(n.h3,{id:"key-features-of-isaac-sim",children:"Key Features of Isaac Sim"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"High-Fidelity Physics"}),": Utilizes PhysX for realistic physics simulation with support for complex materials, friction, and collision properties."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Photorealistic Rendering"}),": Employs RTX ray tracing for realistic lighting and sensor simulation, enabling synthetic data generation for training AI models."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Advanced Sensor Simulation"}),": Provides accurate simulation of cameras, LIDAR, radar, IMU, and other sensors with realistic noise models and imperfections."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"GPU Acceleration"}),": Leverages NVIDIA GPUs for accelerated simulation and rendering, enabling faster-than-real-time execution."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ROS 2 Bridge"}),": Offers seamless integration with ROS 2 systems, allowing hybrid simulation where some components run in Isaac Sim and others in traditional ROS 2 environments."]}),"\n",(0,a.jsx)(n.h3,{id:"isaac-sdk-components",children:"Isaac SDK Components"}),"\n",(0,a.jsx)(n.p,{children:"The Isaac SDK provides a collection of tools and libraries for robotics development:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Sim"}),": The simulation environment"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac ROS"}),": ROS 2 packages for perception and navigation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Apps"}),": Pre-built applications for common robotics tasks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Isaac Utils"}),": Utilities for data processing and visualization"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-example-1-isaac-sim-python-extension",children:"Code Example 1: Isaac Sim Python Extension"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# my_robot_extension.py\n# Purpose: Create a simple robot extension for Isaac Sim\n# Setup Instructions: Place in Isaac Sim extensions directory\n# Run: Isaac Sim Extension Manager\n\nimport omni.ext\nimport omni.kit.commands\nimport omni.usd\nfrom pxr import UsdGeom, Gf, Sdf\nimport carb\n\nclass MyRobotExtension(omni.ext.IExt):\n    def on_startup(self, ext_id):\n        carb.log_info("[my_robot_extension] Startup")\n\n        # Create a simple robot in the scene\n        self._create_simple_robot()\n\n    def _create_simple_robot(self):\n        """Create a simple differential drive robot in the scene"""\n        stage = omni.usd.get_context().get_stage()\n\n        # Create robot root prim\n        robot_prim_path = "/World/MyRobot"\n        robot_prim = stage.DefinePrim(robot_prim_path, "Xform")\n\n        # Create chassis\n        chassis_path = f"{robot_prim_path}/Chassis"\n        chassis_prim = stage.DefinePrim(chassis_path, "Cube")\n        chassis_xform = UsdGeom.Xformable(chassis_prim)\n        chassis_xform.AddTranslateOp().Set(Gf.Vec3f(0, 0, 0.1))\n        chassis_xform.AddScaleOp().Set(Gf.Vec3f(0.5, 0.3, 0.15))\n\n        # Create left wheel\n        left_wheel_path = f"{robot_prim_path}/LeftWheel"\n        left_wheel_prim = stage.DefinePrim(left_wheel_path, "Cylinder")\n        left_wheel_xform = UsdGeom.Xformable(left_wheel_prim)\n        left_wheel_xform.AddTranslateOp().Set(Gf.Vec3f(-0.15, -0.2, 0.05))\n        left_wheel_xform.AddScaleOp().Set(Gf.Vec3f(0.05, 0.05, 0.02))\n\n        # Create right wheel\n        right_wheel_path = f"{robot_prim_path}/RightWheel"\n        right_wheel_prim = stage.DefinePrim(right_wheel_path, "Cylinder")\n        right_wheel_xform = UsdGeom.Xformable(right_wheel_prim)\n        right_wheel_xform.AddTranslateOp().Set(Gf.Vec3f(-0.15, 0.2, 0.05))\n        right_wheel_xform.AddScaleOp().Set(Gf.Vec3f(0.05, 0.05, 0.02))\n\n        carb.log_info("[my_robot_extension] Created simple robot in scene")\n\n    def on_shutdown(self):\n        carb.log_info("[my_robot_extension] Shutdown")\n'})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[my_robot_extension] Startup\n[my_robot_extension] Created simple robot in scene\n[my_robot_extension] Shutdown\n"})}),"\n",(0,a.jsx)(n.h2,{id:"code-example-2-isaac-ros-perception-pipeline",children:"Code Example 2: Isaac ROS Perception Pipeline"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# perception_pipeline.py\n# Purpose: Implement a simple perception pipeline using Isaac ROS\n# Setup Instructions: Isaac ROS packages installed\n# Run: python perception_pipeline.py\n\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass PerceptionPipeline(Node):\n    def __init__(self):\n        super().__init__('perception_pipeline')\n\n        # Create subscriptions\n        self.image_subscription = self.create_subscription(\n            Image,\n            '/front_stereo_camera/left/image_rect_color',\n            self.image_callback,\n            10\n        )\n\n        self.laser_subscription = self.create_subscription(\n            LaserScan,\n            '/scan',\n            self.laser_callback,\n            10\n        )\n\n        # Create publishers for processed data\n        self.obstacle_publisher = self.create_publisher(\n            LaserScan,\n            '/processed_obstacles',\n            10\n        )\n\n        # Initialize CV bridge\n        self.cv_bridge = CvBridge()\n\n        # Parameters\n        self.min_distance_threshold = 0.5  # meters\n        self.max_distance_threshold = 10.0  # meters\n\n        self.get_logger().info('Perception pipeline initialized')\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image data\"\"\"\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform basic image processing\n            height, width, channels = cv_image.shape\n\n            # Detect edges using Canny\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n\n            # Find contours (potential obstacles or features)\n            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            # Draw contours on original image\n            annotated_image = cv_image.copy()\n            cv2.drawContours(annotated_image, contours, -1, (0, 255, 0), 2)\n\n            # Log some information\n            self.get_logger().info(f'Detected {len(contours)} contours in image')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def laser_callback(self, msg):\n        \"\"\"Process incoming LIDAR data\"\"\"\n        try:\n            # Process ranges to detect obstacles\n            valid_ranges = []\n            for i, range_val in enumerate(msg.ranges):\n                if not (np.isnan(range_val) or np.isinf(range_val)):\n                    if self.min_distance_threshold <= range_val <= self.max_distance_threshold:\n                        valid_ranges.append(range_val)\n\n            # Calculate some statistics\n            if len(valid_ranges) > 0:\n                avg_distance = sum(valid_ranges) / len(valid_ranges)\n                min_distance = min(valid_ranges)\n\n                self.get_logger().info(\n                    f'LIDAR: Avg dist {avg_distance:.2f}m, '\n                    f'Min dist {min_distance:.2f}m, '\n                    f'Valid points: {len(valid_ranges)}'\n                )\n\n            # Create processed laser message\n            processed_msg = LaserScan()\n            processed_msg.header = msg.header\n            processed_msg.angle_min = msg.angle_min\n            processed_msg.angle_max = msg.angle_max\n            processed_msg.angle_increment = msg.angle_increment\n            processed_msg.time_increment = msg.time_increment\n            processed_msg.scan_time = msg.scan_time\n            processed_msg.range_min = msg.range_min\n            processed_msg.range_max = msg.range_max\n\n            # Filter ranges for obstacles\n            processed_msg.ranges = [\n                r if self.min_distance_threshold <= r <= self.max_distance_threshold\n                else float('inf')\n                for r in msg.ranges\n            ]\n\n            # Publish processed data\n            self.obstacle_publisher.publish(processed_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing laser scan: {e}')\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    perception_pipeline = PerceptionPipeline()\n\n    try:\n        rclpy.spin(perception_pipeline)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        perception_pipeline.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{children:"[INFO] [perception_pipeline]: Perception pipeline initialized\n[INFO] [perception_pipeline]: Detected 23 contours in image\n[INFO] [perception_pipeline]: LIDAR: Avg dist 2.45m, Min dist 0.67m, Valid points: 1024\n[INFO] [perception_pipeline]: Detected 18 contours in image\n[INFO] [perception_pipeline]: LIDAR: Avg dist 1.98m, Min dist 0.52m, Valid points: 1024\n"})}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"Set up Isaac Sim and create a simple world with a robot model and obstacles"}),"\n",(0,a.jsx)(n.li,{children:"Implement a perception pipeline in Isaac Sim that processes camera and LIDAR data"}),"\n",(0,a.jsx)(n.li,{children:"Connect Isaac Sim to a ROS 2 system and demonstrate data exchange between the two"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(n.p,{children:"Isaac Sim and the Isaac SDK provide a powerful platform for robotics development, particularly for applications requiring high-fidelity simulation and photorealistic rendering. The combination of realistic physics, advanced sensor simulation, and GPU acceleration makes Isaac Sim ideal for developing and testing complex robotics applications, especially those involving perception and AI."}),"\n",(0,a.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,a.jsx)(n.p,{children:"This chapter requires specialized hardware and software:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"NVIDIA GPU with CUDA support (RTX series recommended)"}),"\n",(0,a.jsx)(n.li,{children:"Ubuntu 20.04 or 22.04 LTS"}),"\n",(0,a.jsx)(n.li,{children:"Isaac Sim (available through NVIDIA Developer Program)"}),"\n",(0,a.jsx)(n.li,{children:"Isaac ROS packages"}),"\n",(0,a.jsx)(n.li,{children:"Docker for containerized environments (recommended)"}),"\n",(0,a.jsx)(n.li,{children:"Adequate CPU and RAM for simulation (8+ cores, 32GB+ RAM recommended)"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);