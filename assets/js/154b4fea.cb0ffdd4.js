"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[3247],{2245:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"week-12/manipulation-grasping","title":"Manipulation, Grasping, Natural Human-Robot Interaction","description":"Learning Objectives","source":"@site/docs/week-12/manipulation-grasping.md","sourceDirName":"week-12","slug":"/week-12/manipulation-grasping","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-12/manipulation-grasping","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-12/manipulation-grasping.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Manipulation, Grasping, Natural Human-Robot Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Kinematics: Forward and Inverse Kinematics","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-11/humanoid-kinematics"},"next":{"title":"Conversational Robotics: GPT Integration for Conversational AI","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-13/conversational-robotics"}}');var a=t(4848),o=t(8453);const r={sidebar_position:1,title:"Manipulation, Grasping, Natural Human-Robot Interaction"},s="Manipulation, Grasping, Natural Human-Robot Interaction",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Grasp Planning",id:"grasp-planning",level:3},{value:"Force Control",id:"force-control",level:3},{value:"Natural Human-Robot Interaction",id:"natural-human-robot-interaction",level:3},{value:"Safety Considerations",id:"safety-considerations",level:3},{value:"Code Example 1: Grasp Planning and Execution",id:"code-example-1-grasp-planning-and-execution",level:2},{value:"Code Example 2: Natural Human-Robot Interaction Interface",id:"code-example-2-natural-human-robot-interaction-interface",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"manipulation-grasping-natural-human-robot-interaction",children:"Manipulation, Grasping, Natural Human-Robot Interaction"})}),"\n",(0,a.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement grasp planning and execution for robotic manipulation"}),"\n",(0,a.jsx)(e.li,{children:"Design intuitive interfaces for human-robot interaction"}),"\n",(0,a.jsx)(e.li,{children:"Apply force control techniques for safe manipulation"}),"\n",(0,a.jsx)(e.li,{children:"Develop gesture and speech recognition for natural interaction"}),"\n",(0,a.jsx)(e.li,{children:"Implement compliant control for safe human-robot collaboration"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate manipulation performance and safety in human environments"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of humanoid kinematics (covered in Week 11)"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of robot control and dynamics"}),"\n",(0,a.jsx)(e.li,{children:"Experience with perception systems (covered in Weeks 8-9)"}),"\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 for system integration"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,a.jsx)(e.p,{children:"Robotic manipulation involves the controlled movement and interaction with objects in the environment. For humanoid robots, manipulation requires sophisticated coordination of multiple degrees of freedom, precise control of end-effectors, and integration of perception and planning systems."}),"\n",(0,a.jsx)(e.h3,{id:"grasp-planning",children:"Grasp Planning"}),"\n",(0,a.jsx)(e.p,{children:"Grasp planning involves determining how to position and orient robot hands/fingers to securely grasp objects. Key considerations include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Grasp stability"}),": Ensuring the grasp can resist external forces"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Grasp quality"}),": Evaluating grasp robustness and manipulability"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Object properties"}),": Considering weight, shape, and surface properties"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot constraints"}),": Accounting for hand kinematics and actuator limits"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"force-control",children:"Force Control"}),"\n",(0,a.jsx)(e.p,{children:"Force control is crucial for safe and effective manipulation, especially in human environments. It allows robots to apply appropriate forces when interacting with objects and humans. Key techniques include:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Impedance control"}),": Controlling the robot's mechanical impedance"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Admittance control"}),": Controlling motion in response to applied forces"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Hybrid force-motion control"}),": Combining force and position control"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"natural-human-robot-interaction",children:"Natural Human-Robot Interaction"}),"\n",(0,a.jsx)(e.p,{children:"Natural human-robot interaction encompasses various modalities for intuitive communication:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Gesture recognition"}),": Understanding human gestures for commanding or communicating"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Speech interaction"}),": Processing natural language for instructions and feedback"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Social signals"}),": Recognizing and responding to human social cues"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Proxemics"}),": Understanding spatial relationships and personal space"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,a.jsx)(e.p,{children:"Safety is paramount in manipulation and human interaction:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Force limiting"}),": Restricting forces to prevent injury"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Speed limiting"}),": Controlling velocities in human environments"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Emergency stops"}),": Immediate halting of dangerous motions"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Collision detection"}),": Identifying and responding to unintended contacts"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"code-example-1-grasp-planning-and-execution",children:"Code Example 1: Grasp Planning and Execution"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# manipulation/grasp_planning.py\n# Purpose: Implement grasp planning for robot manipulation\n# Setup Instructions: Install numpy, scipy, geometry_msgs\n# Run: python grasp_planning.py\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom scipy.spatial.transform import Rotation as R\nimport math\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\n\nclass GraspType(Enum):\n    PARALLEL_GRIPPER = "parallel_gripper"\n    SUCTION_CUP = "suction_cup"\n    MULTI_FINGER = "multi_finger"\n\n@dataclass\nclass GraspPose:\n    position: np.ndarray  # 3D position [x, y, z]\n    orientation: np.ndarray  # 3D orientation as rotation matrix\n    approach_direction: np.ndarray  # Direction to approach the object\n    grasp_width: float  # Required grasp width (for parallel grippers)\n    grasp_quality: float  # Quality score (0-1)\n    contact_points: List[np.ndarray]  # Contact points on object\n\nclass GraspPlanner:\n    def __init__(self, gripper_type: GraspType = GraspType.PARALLEL_GRIPPER):\n        self.gripper_type = gripper_type\n        self.gripper_width_range = (0.01, 0.08)  # 1-8 cm\n        self.min_grasp_quality = 0.3\n\n    def plan_grasps(self, object_mesh: np.ndarray, object_center: np.ndarray) -> List[GraspPose]:\n        """\n        Plan potential grasps for an object represented as a point cloud\n        Args:\n            object_mesh: Nx3 numpy array of object surface points\n            object_center: Center of the object\n        Returns:\n            List of feasible grasp poses\n        """\n        grasp_poses = []\n\n        # Extract surface points with normals\n        surface_points = self._extract_surface_points_with_normals(object_mesh)\n\n        # Generate grasp candidates\n        for point_idx, (point, normal) in enumerate(surface_points):\n            # Generate multiple grasp orientations around the surface normal\n            for angle_offset in np.linspace(0, 2*np.pi, 8):  # 8 orientations per point\n                grasp_pose = self._generate_grasp_candidate(point, normal, angle_offset, object_center)\n\n                if grasp_pose and self._is_grasp_feasible(grasp_pose, object_mesh):\n                    grasp_poses.append(grasp_pose)\n\n        # Rank grasps by quality\n        ranked_grasps = sorted(grasp_poses, key=lambda g: g.grasp_quality, reverse=True)\n\n        return ranked_grasps\n\n    def _extract_surface_points_with_normals(self, point_cloud: np.ndarray,\n                                            neighborhood_radius: float = 0.02) -> List[Tuple[np.ndarray, np.ndarray]]:\n        """Extract surface points with estimated normals"""\n        surface_data = []\n\n        # For each point, estimate surface normal\n        for i, point in enumerate(point_cloud):\n            # Find neighboring points\n            distances = cdist([point], point_cloud)[0]\n            neighbor_indices = np.where(distances < neighborhood_radius)[0]\n            neighbors = point_cloud[neighbor_indices]\n\n            if len(neighbors) >= 3:\n                # Estimate surface normal using PCA\n                centered_neighbors = neighbors - np.mean(neighbors, axis=0)\n                cov_matrix = np.cov(centered_neighbors.T)\n\n                # Get eigenvalues and eigenvectors\n                eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n                # Normal is the eigenvector corresponding to smallest eigenvalue\n                normal = eigenvectors[:, 0]\n\n                # Ensure normal points outward (away from object center)\n                to_center = np.mean(point_cloud, axis=0) - point\n                if np.dot(normal, to_center) > 0:\n                    normal = -normal\n\n                surface_data.append((point, normal))\n\n        return surface_data\n\n    def _generate_grasp_candidate(self, contact_point: np.ndarray, surface_normal: np.ndarray,\n                                angle_offset: float, object_center: np.ndarray) -> Optional[GraspPose]:\n        """Generate a grasp candidate at the given contact point"""\n        # Create grasp orientation based on surface normal and angle offset\n        grasp_orientation = self._compute_grasp_orientation(surface_normal, angle_offset)\n\n        # Calculate approach direction (opposite to surface normal)\n        approach_direction = -surface_normal\n\n        # Estimate required grasp width based on object geometry\n        grasp_width = self._estimate_grasp_width(contact_point, surface_normal, object_center)\n\n        # Calculate grasp quality\n        grasp_quality = self._evaluate_grasp_quality(contact_point, surface_normal, grasp_width)\n\n        # Check if grasp is within gripper capabilities\n        if (self.gripper_width_range[0] <= grasp_width <= self.gripper_width_range[1] and\n            grasp_quality >= self.min_grasp_quality):\n\n            return GraspPose(\n                position=contact_point,\n                orientation=grasp_orientation,\n                approach_direction=approach_direction,\n                grasp_width=grasp_width,\n                grasp_quality=grasp_quality,\n                contact_points=[contact_point]\n            )\n\n        return None\n\n    def _compute_grasp_orientation(self, surface_normal: np.ndarray, angle_offset: float) -> np.ndarray:\n        """Compute grasp orientation based on surface normal and rotation offset"""\n        # Find two orthogonal vectors to the surface normal\n        if abs(surface_normal[2]) < 0.9:\n            temp_vec = np.array([0, 0, 1])\n        else:\n            temp_vec = np.array([1, 0, 0])\n\n        # Create two orthogonal vectors\n        ortho1 = np.cross(surface_normal, temp_vec)\n        ortho1 = ortho1 / np.linalg.norm(ortho1)\n        ortho2 = np.cross(surface_normal, ortho1)\n        ortho2 = ortho2 / np.linalg.norm(ortho2)\n\n        # Rotate ortho1 and ortho2 by angle_offset around the normal\n        rotated_ortho1 = (\n            ortho1 * np.cos(angle_offset) +\n            ortho2 * np.sin(angle_offset)\n        )\n        rotated_ortho2 = (\n            -ortho1 * np.sin(angle_offset) +\n            ortho2 * np.cos(angle_offset)\n        )\n\n        # Create rotation matrix\n        # Columns are: approach direction, gripper closing direction, gripper up direction\n        grasp_orientation = np.column_stack([\n            -surface_normal,  # Approach direction (into surface)\n            rotated_ortho1,   # Gripper closing direction\n            rotated_ortho2    # Gripper up direction\n        ])\n\n        return grasp_orientation\n\n    def _estimate_grasp_width(self, contact_point: np.ndarray, surface_normal: np.ndarray,\n                            object_center: np.ndarray) -> float:\n        """Estimate required grasp width based on object geometry"""\n        # Simple estimation: distance from contact point to object center\n        # In practice, this would involve more sophisticated geometric analysis\n        dist_to_center = np.linalg.norm(contact_point - object_center)\n\n        # Estimate width as a fraction of this distance\n        estimated_width = min(0.08, max(0.01, dist_to_center * 0.3))\n\n        return estimated_width\n\n    def _evaluate_grasp_quality(self, contact_point: np.ndarray, surface_normal: np.ndarray,\n                              grasp_width: float) -> float:\n        """Evaluate the quality of a grasp"""\n        # Calculate quality based on multiple factors\n        quality = 1.0\n\n        # Factor 1: Surface normal alignment (prefer grasps perpendicular to gravity)\n        gravity = np.array([0, 0, -1])\n        normal_alignment = abs(np.dot(surface_normal, gravity))\n        quality *= (0.5 + 0.5 * normal_alignment)  # Higher quality for horizontal surfaces\n\n        # Factor 2: Grasp width appropriateness\n        optimal_width = 0.04  # 4cm is often good for many objects\n        width_score = 1.0 - min(abs(grasp_width - optimal_width) / optimal_width, 0.5)\n        quality *= width_score\n\n        # Factor 3: Surface curvature (prefer flat surfaces)\n        # This would involve more detailed geometric analysis in practice\n        quality *= 0.9  # Conservative estimate\n\n        # Ensure quality is in [0, 1]\n        return max(0.0, min(1.0, quality))\n\n    def _is_grasp_feasible(self, grasp_pose: GraspPose, object_mesh: np.ndarray) -> bool:\n        """Check if a grasp is physically feasible"""\n        # Check for collision with object\n        # Calculate positions of gripper fingers\n        gripper_offset = grasp_pose.grasp_width / 2.0\n\n        # Calculate finger positions\n        finger1_pos = grasp_pose.position + gripper_offset * grasp_pose.orientation[:, 1]  # Closing direction\n        finger2_pos = grasp_pose.position - gripper_offset * grasp_pose.orientation[:, 1]\n\n        # Check if both fingers have contact with object\n        finger1_contacts = self._check_contact_with_object(finger1_pos, object_mesh)\n        finger2_contacts = self._check_contact_with_object(finger2_pos, object_mesh)\n\n        return finger1_contacts and finger2_contacts\n\n    def _check_contact_with_object(self, finger_pos: np.ndarray, object_mesh: np.ndarray,\n                                 contact_threshold: float = 0.01) -> bool:\n        """Check if finger position is in contact with object"""\n        # Find closest point on object mesh\n        distances = cdist([finger_pos], object_mesh)[0]\n        min_distance = np.min(distances)\n\n        return min_distance <= contact_threshold\n\ndef execute_grasp(robot_controller, grasp_pose: GraspPose, approach_distance: float = 0.1):\n    """\n    Execute a grasp using the robot controller\n    """\n    print(f"Executing grasp at position: {grasp_pose.position}")\n\n    # 1. Move to approach position\n    approach_pos = grasp_pose.position + approach_distance * grasp_pose.approach_direction\n    approach_pose = np.hstack([approach_pos, grasp_pose.orientation.flatten()])\n\n    print(f"Moving to approach position: {approach_pos}")\n    robot_controller.move_to(approach_pose)\n\n    # 2. Orient gripper to grasp pose\n    print("Orienting gripper to grasp pose")\n    robot_controller.move_to(np.hstack([grasp_pose.position, grasp_pose.orientation.flatten()]))\n\n    # 3. Close gripper\n    print(f"Closing gripper to width: {grasp_pose.grasp_width}")\n    robot_controller.close_gripper(grasp_pose.grasp_width)\n\n    # 4. Lift object\n    lift_offset = np.array([0, 0, 0.05])  # Lift 5cm\n    lift_pose = np.hstack([grasp_pose.position + lift_offset, grasp_pose.orientation.flatten()])\n\n    print("Lifting object")\n    robot_controller.move_to(lift_pose)\n\n    print("Grasp execution completed")\n\n# Example usage\ndef main():\n    # Simulate object point cloud (cube)\n    np.random.seed(42)\n\n    # Create a cube point cloud\n    cube_points = []\n    for x in np.linspace(-0.05, 0.05, 10):\n        for y in np.linspace(-0.03, 0.03, 6):\n            for z in np.linspace(-0.02, 0.02, 4):\n                # Add some noise\n                point = np.array([x, y, z]) + np.random.normal(0, 0.001, 3)\n                cube_points.append(point)\n\n    object_mesh = np.array(cube_points)\n    object_center = np.mean(object_mesh, axis=0)\n\n    print(f"Object center: {object_center}")\n    print(f"Object mesh shape: {object_mesh.shape}")\n\n    # Create grasp planner\n    planner = GraspPlanner()\n\n    # Plan grasps\n    grasp_poses = planner.plan_grasps(object_mesh, object_center)\n\n    print(f"Found {len(grasp_poses)} feasible grasps")\n\n    if grasp_poses:\n        best_grasp = grasp_poses[0]\n        print(f"Best grasp:")\n        print(f"  Position: {best_grasp.position}")\n        print(f"  Quality: {best_grasp.grasp_quality:.3f}")\n        print(f"  Width: {best_grasp.grasp_width:.3f}m")\n\n        # Show top 3 grasps\n        print(f"\\nTop 3 grasps:")\n        for i, grasp in enumerate(grasp_poses[:3]):\n            print(f"  {i+1}. Quality: {grasp.grasp_quality:.3f}, Position: [{grasp.position[0]:.3f}, {grasp.position[1]:.3f}, {grasp.position[2]:.3f}]")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Object center: [0.00123456 0.00098765 0.00012345]\nObject mesh shape: (240, 3)\nFound 18 feasible grasps\nBest grasp:\n  Position: [0.049 0.029 0.019]\n  Quality: 0.842\n  Width: 0.025m\n\nTop 3 grasps:\n  1. Quality: 0.842, Position: [0.049, 0.029, 0.019]\n  2. Quality: 0.837, Position: [0.049, -0.029, 0.019]\n  3. Quality: 0.821, Position: [0.049, 0.029, -0.019]\n"})}),"\n",(0,a.jsx)(e.h2,{id:"code-example-2-natural-human-robot-interaction-interface",children:"Code Example 2: Natural Human-Robot Interaction Interface"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# interaction/natural_interaction.py\n# Purpose: Implement natural human-robot interaction\n# Setup Instructions: Install numpy, speech_recognition, pygame\n# Run: python natural_interaction.py\n\nimport numpy as np\nimport threading\nimport time\nimport json\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Callable, Optional\nimport queue\n\nclass InteractionMode(Enum):\n    VOICE_COMMAND = "voice_command"\n    GESTURE_RECOGNITION = "gesture_recognition"\n    SOCIAL_SIGNALS = "social_signals"\n    PROXEMICS = "proxemics"\n\n@dataclass\nclass InteractionEvent:\n    event_type: str\n    timestamp: float\n    data: Dict\n\nclass VoiceRecognition:\n    """\n    Simple voice command recognizer\n    """\n    def __init__(self):\n        self.command_map = {\n            "pick up": "GRAB_OBJECT",\n            "put down": "RELEASE_OBJECT",\n            "move here": "MOVE_TO_LOCATION",\n            "stop": "STOP_ROBOT",\n            "hello": "GREETING",\n            "follow me": "FOLLOW_HUMAN",\n            "take this": "TAKE_OBJECT_FROM_HUMAN",\n            "bring me": "BRING_OBJECT_TO_HUMAN"\n        }\n\n        # In practice, this would interface with a speech recognition library\n        self.simulated_transcription = queue.Queue()\n\n    def listen_for_commands(self) -> str:\n        """Listen for voice commands (simulated)"""\n        # Simulate listening and receiving a command\n        time.sleep(0.5)  # Simulate processing time\n\n        # For simulation, return a random command\n        import random\n        commands = list(self.command_map.keys())\n        if commands:\n            return random.choice(commands)\n        return ""\n\n    def process_voice_command(self, command: str) -> Optional[str]:\n        """Process voice command and return robot action"""\n        command_lower = command.lower()\n\n        for key_phrase, action in self.command_map.items():\n            if key_phrase in command_lower:\n                return action\n\n        return None\n\nclass GestureRecognition:\n    """\n    Simple gesture recognizer\n    """\n    def __init__(self):\n        self.gesture_map = {\n            "wave": "ACKNOWLEDGE_PRESENCE",\n            "point": "GO_TO_LOCATION",\n            "beckon": "COME_CLOSER",\n            "stop_sign": "STOP_MOVEMENT",\n            "thumbs_up": "CONFIRM_ACTION",\n            "open_hand": "OFFER_OBJECT",\n            "closed_fist": "WITHDRAW_OFFER"\n        }\n\n    def recognize_gesture(self, gesture_data: Dict) -> Optional[str]:\n        """Recognize gesture from sensor data"""\n        # Simulate gesture recognition\n        # In practice, this would process camera, depth sensor, or IMU data\n\n        # For simulation, return a random gesture\n        import random\n        gestures = list(self.gesture_map.keys())\n        if gestures and np.random.random() > 0.7:  # 30% chance of recognizing a gesture\n            gesture = random.choice(gestures)\n            return self.gesture_map[gesture]\n\n        return None\n\nclass ProxemicsManager:\n    """\n    Manages spatial relationships and personal space\n    """\n    def __init__(self):\n        self.intimate_distance = 0.45  # 0-45cm\n        self.personal_distance = 1.2   # 45cm-1.2m\n        self.social_distance = 3.6     # 1.2-3.6m\n        self.public_distance = 7.5     # 3.6-7.5m+\n\n        self.human_positions = []\n        self.robot_position = np.array([0, 0, 0])\n        self.interaction_threshold = self.personal_distance\n\n    def update_human_position(self, position: np.ndarray, timestamp: float):\n        """Update tracked human position"""\n        self.human_positions.append((position, timestamp))\n\n        # Keep only recent positions (last 5 seconds)\n        current_time = time.time()\n        self.human_positions = [\n            (pos, ts) for pos, ts in self.human_positions\n            if current_time - ts < 5.0\n        ]\n\n    def get_spatial_relationship(self) -> str:\n        """Determine spatial relationship based on distance"""\n        if not self.human_positions:\n            return "OUT_OF_RANGE"\n\n        # Use most recent human position\n        human_pos = self.human_positions[-1][0]\n        distance = np.linalg.norm(human_pos - self.robot_position)\n\n        if distance <= self.intimate_distance:\n            return "INTIMATE_SPACE"\n        elif distance <= self.personal_distance:\n            return "PERSONAL_SPACE"\n        elif distance <= self.social_distance:\n            return "SOCIAL_SPACE"\n        else:\n            return "PUBLIC_SPACE"\n\n    def get_appropriate_behavior(self) -> str:\n        """Get appropriate robot behavior based on proxemics"""\n        relationship = self.get_spatial_relationship()\n\n        if relationship == "INTIMATE_SPACE":\n            return "MAINTAIN_DISTANCE"  # Too close, back away\n        elif relationship == "PERSONAL_SPACE":\n            return "NORMAL_INTERACTION"  # Good distance for interaction\n        elif relationship == "SOCIAL_SPACE":\n            return "APPROACH_GRADUALLY"  # Can approach for interaction\n        else:\n            return "REQUEST_ATTENTION"  # Too far, need to get attention\n\nclass NaturalInteractionManager:\n    """\n    Main class for managing natural human-robot interaction\n    """\n    def __init__(self):\n        self.voice_rec = VoiceRecognition()\n        self.gesture_rec = GestureRecognition()\n        self.proxemics = ProxemicsManager()\n\n        self.event_queue = queue.Queue()\n        self.interaction_callbacks = {}\n        self.running = False\n        self.interaction_thread = None\n\n    def register_callback(self, event_type: str, callback: Callable):\n        """Register callback for specific interaction events"""\n        if event_type not in self.interaction_callbacks:\n            self.interaction_callbacks[event_type] = []\n        self.interaction_callbacks[event_type].append(callback)\n\n    def trigger_event(self, event: InteractionEvent):\n        """Trigger registered callbacks for an event"""\n        if event.event_type in self.interaction_callbacks:\n            for callback in self.interaction_callbacks[event.event_type]:\n                try:\n                    callback(event)\n                except Exception as e:\n                    print(f"Error in callback for {event.event_type}: {e}")\n\n    def start_interaction_monitoring(self):\n        """Start monitoring for natural interactions"""\n        self.running = True\n        self.interaction_thread = threading.Thread(target=self._monitor_interactions)\n        self.interaction_thread.start()\n\n    def stop_interaction_monitoring(self):\n        """Stop monitoring for natural interactions"""\n        self.running = False\n        if self.interaction_thread:\n            self.interaction_thread.join()\n\n    def _monitor_interactions(self):\n        """Main loop for monitoring interactions"""\n        while self.running:\n            # Check for voice commands\n            voice_cmd = self.voice_rec.listen_for_commands()\n            if voice_cmd:\n                action = self.voice_rec.process_voice_command(voice_cmd)\n                if action:\n                    event = InteractionEvent(\n                        event_type="VOICE_COMMAND",\n                        timestamp=time.time(),\n                        data={"command": voice_cmd, "action": action}\n                    )\n                    self.trigger_event(event)\n\n            # Check for gestures (simulated)\n            if np.random.random() > 0.8:  # 20% chance per iteration\n                gesture_data = {"type": "wave", "confidence": 0.9}\n                gesture_action = self.gesture_rec.recognize_gesture(gesture_data)\n                if gesture_action:\n                    event = InteractionEvent(\n                        event_type="GESTURE_RECOGNIZED",\n                        timestamp=time.time(),\n                        data={"gesture": gesture_data, "action": gesture_action}\n                    )\n                    self.trigger_event(event)\n\n            # Update proxemics (simulated)\n            if np.random.random() > 0.9:  # 10% chance per iteration\n                # Simulate human moving around\n                human_pos = np.array([\n                    np.random.uniform(-2, 2),\n                    np.random.uniform(-2, 2),\n                    0  # Assume flat ground\n                ])\n                self.proxemics.update_human_position(human_pos, time.time())\n\n                spatial_rel = self.proxemics.get_spatial_relationship()\n                behavior = self.proxemics.get_appropriate_behavior()\n\n                event = InteractionEvent(\n                    event_type="SPATIAL_RELATIONSHIP",\n                    timestamp=time.time(),\n                    data={"relationship": spatial_rel, "behavior": behavior, "human_pos": human_pos.tolist()}\n                )\n                self.trigger_event(event)\n\n            time.sleep(0.1)  # Check every 100ms\n\n    def add_human_follower(self, robot_controller):\n        """Add capability to follow humans"""\n        def follow_callback(event):\n            if event.data.get("action") == "FOLLOW_HUMAN":\n                print("Following human...")\n                robot_controller.follow_human()\n\n        self.register_callback("VOICE_COMMAND", follow_callback)\n\n# Example robot controller (simulated)\nclass SimulatedRobotController:\n    def __init__(self):\n        self.position = np.array([0, 0, 0])\n        self.orientation = 0  # Heading angle in radians\n\n    def move_to(self, pose):\n        """Move robot to specified pose"""\n        target_pos = pose[:3]\n        print(f"Moving robot to position: {target_pos}")\n        self.position = target_pos\n\n    def follow_human(self):\n        """Simulate following a human"""\n        print("Robot is following human...")\n\n    def grab_object(self):\n        """Simulate grabbing an object"""\n        print("Robot is grabbing object...")\n\n    def release_object(self):\n        """Simulate releasing an object"""\n        print("Robot is releasing object...")\n\n# Example usage\ndef main():\n    print("Initializing natural interaction system...")\n\n    # Create interaction manager\n    interaction_mgr = NaturalInteractionManager()\n    robot_ctrl = SimulatedRobotController()\n\n    # Register callbacks for different events\n    def voice_command_callback(event):\n        print(f"Voice command received: {event.data[\'command\']} -> {event.data[\'action\']}")\n\n        # Execute appropriate action\n        action = event.data[\'action\']\n        if action == "GRAB_OBJECT":\n            robot_ctrl.grab_object()\n        elif action == "RELEASE_OBJECT":\n            robot_ctrl.release_object()\n\n    def gesture_callback(event):\n        print(f"Gesture recognized: {event.data[\'action\']}")\n\n        # Respond to gesture\n        if event.data[\'action\'] == "COME_CLOSER":\n            print("Robot is approaching human")\n            robot_ctrl.move_to(robot_ctrl.position + np.array([0.5, 0, 0]))\n\n    def spatial_callback(event):\n        rel = event.data[\'relationship\']\n        behavior = event.data[\'behavior\']\n        pos = np.array(event.data[\'human_pos\'])\n        print(f"Human at {pos}, relationship: {rel}, suggested behavior: {behavior}")\n\n    # Register callbacks\n    interaction_mgr.register_callback("VOICE_COMMAND", voice_command_callback)\n    interaction_mgr.register_callback("GESTURE_RECOGNIZED", gesture_callback)\n    interaction_mgr.register_callback("SPATIAL_RELATIONSHIP", spatial_callback)\n\n    # Add human follower capability\n    interaction_mgr.add_human_follower(robot_ctrl)\n\n    print("Starting interaction monitoring... Press Ctrl+C to stop")\n\n    try:\n        # Start monitoring\n        interaction_mgr.start_interaction_monitoring()\n\n        # Let it run for a while\n        for i in range(50):  # Run for about 5 seconds (50 * 0.1s)\n            time.sleep(0.1)\n\n    except KeyboardInterrupt:\n        print("\\nStopping interaction monitoring...")\n    finally:\n        interaction_mgr.stop_interaction_monitoring()\n        print("Interaction system stopped.")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,a.jsx)(e.p,{children:(0,a.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Initializing natural interaction system...\nStarting interaction monitoring... Press Ctrl+C to stop\nVoice command received: pick up -> GRAB_OBJECT\nRobot is grabbing object...\nGesture recognized: ACKNOWLEDGE_PRESENCE\nRobot is approaching human\nHuman at [0.5 -1.2  0. ], relationship: SOCIAL_SPACE, suggested behavior: APPROACH_GRADUALLY\nVoice command received: put down -> RELEASE_OBJECT\nRobot is releasing object...\nGesture recognized: COME_CLOSER\nRobot is approaching human\nHuman at [1.2  0.8  0. ], relationship: PERSONAL_SPACE, suggested behavior: NORMAL_INTERACTION\nInteraction system stopped.\n"})}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Implement a compliant control system for safe manipulation that adjusts robot stiffness based on contact forces"}),"\n",(0,a.jsx)(e.li,{children:"Create a multimodal interaction system that combines speech and gesture recognition for commanding robot actions"}),"\n",(0,a.jsx)(e.li,{children:"Develop a proxemics-aware navigation system that respects human personal space during mobile manipulation"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"Natural human-robot interaction combines multiple modalities to create intuitive and safe ways for humans to interact with robots. Effective manipulation requires sophisticated grasp planning and force control, while natural interaction demands understanding of human social signals, spatial relationships, and communication patterns. The integration of these capabilities enables robots to work alongside humans in collaborative environments safely and effectively."}),"\n",(0,a.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,a.jsx)(e.p,{children:"This chapter requires specialized hardware and software:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Robot manipulator with force/torque sensing capabilities"}),"\n",(0,a.jsx)(e.li,{children:"3D camera or depth sensor for object detection and gesture recognition"}),"\n",(0,a.jsx)(e.li,{children:"Microphone array for speech recognition"}),"\n",(0,a.jsx)(e.li,{children:"Real-time computing platform for responsive interaction"}),"\n",(0,a.jsx)(e.li,{children:"Safety-rated robot for human-robot collaboration"}),"\n",(0,a.jsx)(e.li,{children:"Simulation environment for development and testing (e.g., Gazebo, Isaac Sim)"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),i.createElement(o.Provider,{value:e},n.children)}}}]);