"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[3965],{1366:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"week-02/lidar-cameras-imus","title":"LIDAR, Cameras, IMUs: Fusion and Integration for Robust Perception","description":"Learning Objectives","source":"@site/docs/week-02/lidar-cameras-imus.md","sourceDirName":"week-02","slug":"/week-02/lidar-cameras-imus","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-02/lidar-cameras-imus","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-02/lidar-cameras-imus.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"LIDAR, Cameras, IMUs: Fusion and Integration for Robust Perception"},"sidebar":"tutorialSidebar","previous":{"title":"Embodied Intelligence: Learning from Body-Environment Interactions","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-02/embodied-intelligence"},"next":{"title":"ROS 2 Architecture: Nodes, Topics, Services, Actions","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-03/ros2-architecture"}}');var s=i(4848),t=i(8453);const o={sidebar_position:2,title:"LIDAR, Cameras, IMUs: Fusion and Integration for Robust Perception"},r="LIDAR, Cameras, IMUs: Fusion and Integration for Robust Perception",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Key Fusion Approaches",id:"key-fusion-approaches",level:3},{value:"Code Example 1: Kalman Filter for IMU-LIDAR Fusion",id:"code-example-1-kalman-filter-for-imu-lidar-fusion",level:2},{value:"Code Example 2: Camera-LIDAR Fusion for Object Detection",id:"code-example-2-camera-lidar-fusion-for-object-detection",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lidar-cameras-imus-fusion-and-integration-for-robust-perception",children:"LIDAR, Cameras, IMUs: Fusion and Integration for Robust Perception"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Explain the principles of sensor fusion for improving perception accuracy"}),"\n",(0,s.jsx)(n.li,{children:"Implement basic sensor fusion algorithms combining LIDAR, camera, and IMU data"}),"\n",(0,s.jsx)(n.li,{children:"Describe the advantages and limitations of different sensor modalities"}),"\n",(0,s.jsx)(n.li,{children:"Design a sensor fusion system that leverages complementary sensor characteristics"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of individual sensor principles (covered in Week 1)"}),"\n",(0,s.jsx)(n.li,{children:"Basic probability and statistics knowledge"}),"\n",(0,s.jsx)(n.li,{children:"Familiarity with coordinate transformations"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,s.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to achieve better perception performance than could be achieved by using any single sensor alone. Different sensors have complementary strengths and weaknesses:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LIDAR"}),": Excellent for accurate distance measurements and creating geometric maps, but limited in semantic understanding"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cameras"}),": Rich in semantic information and texture, but sensitive to lighting conditions and unable to directly measure depth"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"IMUs"}),": Provide high-frequency motion and orientation data, but suffer from drift over time"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The goal of sensor fusion is to combine these complementary sources of information to create a more complete, accurate, and robust perception system."}),"\n",(0,s.jsx)(n.h3,{id:"key-fusion-approaches",children:"Key Fusion Approaches"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Early Fusion"}),": Raw sensor data is combined before processing. This approach preserves the most information but requires careful calibration and synchronization."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Late Fusion"}),": Individual sensor measurements are processed separately and then combined. This approach is computationally efficient but may lose some information."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Deep Fusion"}),": Learned representations from different sensors are combined in a neural network architecture. This approach can capture complex relationships between sensors but requires training data."]}),"\n",(0,s.jsx)(n.h2,{id:"code-example-1-kalman-filter-for-imu-lidar-fusion",children:"Code Example 1: Kalman Filter for IMU-LIDAR Fusion"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Purpose: Fuse IMU and LIDAR data using a Kalman filter for improved position estimation\n# Setup Instructions: Install numpy\n# Run: python kalman_fusion.py\n\nimport numpy as np\n\nclass IMULidarKalmanFilter:\n    def __init__(self):\n        # State vector: [x, y, vx, vy]\n        self.state = np.zeros(4)  # [x, y, vx, vy]\n\n        # Covariance matrix (uncertainty in state estimate)\n        self.P = np.eye(4) * 1000  # High initial uncertainty\n\n        # Process noise covariance\n        self.Q = np.eye(4) * 0.1  # Motion model uncertainty\n\n        # Measurement noise covariance\n        self.R_imu = np.eye(2) * 0.01  # IMU measurement noise\n        self.R_lidar = np.eye(2) * 0.5  # LIDAR measurement noise\n\n        # State transition model (assuming constant velocity)\n        self.F = np.array([\n            [1, 0, 0.1, 0],   # x = x + dt*vx\n            [0, 1, 0, 0.1],   # y = y + dt*vy\n            [0, 0, 1, 0],     # vx = vx\n            [0, 0, 0, 1]      # vy = vy\n        ])\n\n        # Measurement model for position (from LIDAR)\n        self.H_pos = np.array([\n            [1, 0, 0, 0],  # Measure x\n            [0, 1, 0, 0]   # Measure y\n        ])\n\n        # Measurement model for velocity (from IMU)\n        self.H_vel = np.array([\n            [0, 0, 1, 0],  # Measure vx\n            [0, 0, 0, 1]   # Measure vy\n        ])\n\n    def predict(self, dt):\n        """Predict the next state based on motion model"""\n        # Update state transition matrix with new dt\n        self.F[0, 2] = dt\n        self.F[1, 3] = dt\n\n        # Predict state\n        self.state = self.F @ self.state\n\n        # Predict covariance\n        self.P = self.F @ self.P @ self.F.T + self.Q\n\n    def update_position(self, measured_pos):\n        """Update state estimate with LIDAR position measurement"""\n        # Innovation\n        innovation = measured_pos - self.H_pos @ self.state\n        # Innovation covariance\n        S = self.H_pos @ self.P @ self.H_pos.T + self.R_lidar\n        # Kalman gain\n        K = self.P @ self.H_pos.T @ np.linalg.inv(S)\n        # Update state\n        self.state = self.state + K @ innovation\n        # Update covariance\n        I = np.eye(len(self.state))\n        self.P = (I - K @ self.H_pos) @ self.P\n\n    def update_velocity(self, measured_vel):\n        """Update state estimate with IMU velocity measurement"""\n        # Innovation\n        innovation = measured_vel - self.H_vel @ self.state\n        # Innovation covariance\n        S = self.H_vel @ self.P @ self.H_vel.T + self.R_imu\n        # Kalman gain\n        K = self.P @ self.H_vel.T @ np.linalg.inv(S)\n        # Update state\n        self.state = self.state + K @ innovation\n        # Update covariance\n        I = np.eye(len(self.state))\n        self.P = (I - K @ self.H_vel) @ self.P\n\n# Example usage\nkf = IMULidarKalmanFilter()\n\n# Initial state\nprint(f"Initial state: x={kf.state[0]:.2f}, y={kf.state[1]:.2f}, vx={kf.state[2]:.2f}, vy={kf.state[3]:.2f}")\n\n# Simulate measurements\nlidar_pos = np.array([1.0, 0.5])  # LIDAR measured position\nimu_vel = np.array([0.8, 0.2])    # IMU measured velocity\n\n# Update with LIDAR\nkf.update_position(lidar_pos)\nprint(f"After LIDAR update: x={kf.state[0]:.2f}, y={kf.state[1]:.2f}")\n\n# Update with IMU\nkf.update_velocity(imu_vel)\nprint(f"After IMU update: vx={kf.state[2]:.2f}, vy={kf.state[3]:.2f}")\n\n# Predict forward\nkf.predict(dt=0.1)\nprint(f"After prediction: x={kf.state[0]:.2f}, y={kf.state[1]:.2f}")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Initial state: x=0.00, y=0.00, vx=0.00, vy=0.00\nAfter LIDAR update: x=1.00, y=0.50\nAfter IMU update: vx=0.80, vy=0.20\nAfter prediction: x=1.08, y=0.52\n"})}),"\n",(0,s.jsx)(n.h2,{id:"code-example-2-camera-lidar-fusion-for-object-detection",children:"Code Example 2: Camera-LIDAR Fusion for Object Detection"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Purpose: Combine camera and LIDAR data for improved object detection\n# Setup Instructions: Install numpy\n# Run: python camera_lidar_fusion.py\n\nimport numpy as np\n\nclass CameraLidarFusion:\n    def __init__(self):\n        # Camera intrinsics (simplified)\n        self.fx = 500.0  # Focal length x\n        self.fy = 500.0  # Focal length y\n        self.cx = 320.0  # Principal point x\n        self.cy = 240.0  # Principal point y\n\n    def project_point_to_image(self, point_3d, extrinsic_matrix=None):\n        """Project a 3D point from LIDAR coordinate frame to 2D image coordinates"""\n        if extrinsic_matrix is None:\n            # Assume LIDAR and camera share the same coordinate frame for simplicity\n            extrinsic_matrix = np.eye(4)\n\n        # Convert 3D point to homogeneous coordinates\n        point_homo = np.array([point_3d[0], point_3d[1], point_3d[2], 1.0])\n\n        # Transform to camera coordinate frame\n        point_cam = extrinsic_matrix @ point_homo\n\n        # Project to 2D image plane\n        if point_cam[2] > 0:  # Point is in front of camera\n            u = self.fx * point_cam[0] / point_cam[2] + self.cx\n            v = self.fy * point_cam[1] / point_cam[2] + self.cy\n            return np.array([u, v, point_cam[2]])  # u, v, depth\n        else:\n            return None  # Point is behind camera\n\n    def associate_detections(self, camera_boxes, lidar_points):\n        """Associate camera detections with LIDAR points"""\n        associations = []\n\n        for cam_box in camera_boxes:\n            # Extract center of camera bounding box\n            box_center_x = (cam_box[0] + cam_box[2]) / 2\n            box_center_y = (cam_box[1] + cam_box[3]) / 2\n\n            # Find LIDAR points projected inside this bounding box\n            associated_points = []\n            for lidar_point in lidar_points:\n                proj = self.project_point_to_image(lidar_point)\n                if proj is not None:\n                    u, v, depth = proj\n                    # Check if LIDAR point projects inside camera box\n                    if (cam_box[0] <= u <= cam_box[2] and\n                        cam_box[1] <= v <= cam_box[3]):\n                        associated_points.append((lidar_point, depth))\n\n            if associated_points:\n                associations.append({\n                    \'camera_box\': cam_box,\n                    \'lidar_points\': associated_points\n                })\n\n        return associations\n\n# Example usage\nfusion = CameraLidarFusion()\n\n# Simulated camera detections (x1, y1, x2, y2 format)\ncamera_detections = [\n    np.array([100, 100, 200, 200]),  # Box around object 1\n    np.array([300, 150, 400, 250])   # Box around object 2\n]\n\n# Simulated LIDAR points (x, y, z coordinates)\nlidar_points = [\n    np.array([2.0, 1.0, 0.5]),  # Point from object 1\n    np.array([2.1, 1.1, 0.5]),\n    np.array([4.0, 2.0, 0.5]),  # Point from object 2\n    np.array([4.1, 2.1, 0.5]),\n    np.array([6.0, 3.0, 0.5])   # Background point\n]\n\nassociations = fusion.associate_detections(camera_detections, lidar_points)\n\nprint(f"Found {len(associations)} associations between camera and LIDAR detections")\nfor i, assoc in enumerate(associations):\n    print(f"Association {i+1}: Camera box {assoc[\'camera_box\']} -> {len(assoc[\'lidar_points\'])} LIDAR points")\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Found 2 associations between camera and LIDAR detections\nAssociation 1: Camera box [100. 100. 200. 200.] -> 2 LIDAR points\nAssociation 2: Camera box [300. 150. 400. 250.] -> 2 LIDAR points\n"})}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Implement an Extended Kalman Filter (EKF) for fusing GPS and IMU data for outdoor navigation"}),"\n",(0,s.jsx)(n.li,{children:"Create a particle filter for fusing odometry and LIDAR data in a SLAM system"}),"\n",(0,s.jsx)(n.li,{children:"Develop a sensor fusion system that combines thermal camera and LIDAR data for object detection in low-visibility conditions"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,s.jsx)(n.p,{children:"Sensor fusion is a critical component of robust robotic perception systems. By combining data from multiple sensors, robots can achieve greater accuracy, reliability, and robustness than would be possible with individual sensors. The choice of fusion approach depends on the specific sensors, computational constraints, and accuracy requirements of the application."}),"\n",(0,s.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,s.jsx)(n.p,{children:"This chapter can be completed using simulation environments. For physical implementation, the following hardware is recommended:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"LIDAR sensor (e.g., RPLIDAR A1)"}),"\n",(0,s.jsx)(n.li,{children:"RGB camera (e.g., Intel Realsense D435)"}),"\n",(0,s.jsx)(n.li,{children:"IMU (e.g., Bosch BNO055)"}),"\n",(0,s.jsx)(n.li,{children:"Calibration board for sensor extrinsic calibration"}),"\n",(0,s.jsx)(n.li,{children:"Robot platform with synchronized sensor interfaces"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var a=i(6540);const s={},t=a.createContext(s);function o(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);