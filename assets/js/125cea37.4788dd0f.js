"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[6738],{8453:(n,e,a)=>{a.d(e,{R:()=>o,x:()=>s});var i=a(6540);const r={},t=i.createContext(r);function o(n){const e=i.useContext(t);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:o(n.components),i.createElement(t.Provider,{value:e},n.children)}},9906:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"week-10/reinforcement-learning-transfer","title":"Reinforcement Learning and Sim-to-Real Transfer","description":"Learning Objectives","source":"@site/docs/week-10/reinforcement-learning-transfer.md","sourceDirName":"week-10","slug":"/week-10/reinforcement-learning-transfer","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-10/reinforcement-learning-transfer","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-10/reinforcement-learning-transfer.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Reinforcement Learning and Sim-to-Real Transfer"},"sidebar":"tutorialSidebar","previous":{"title":"AI-Powered Perception and Manipulation","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-09/ai-powered-perception"},"next":{"title":"Humanoid Kinematics: Forward and Inverse Kinematics","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-11/humanoid-kinematics"}}');var r=a(4848),t=a(8453);const o={sidebar_position:1,title:"Reinforcement Learning and Sim-to-Real Transfer"},s="Reinforcement Learning and Sim-to-Real Transfer",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Sim-to-Real Transfer Challenge",id:"sim-to-real-transfer-challenge",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Domain Adaptation",id:"domain-adaptation",level:3},{value:"Code Example 1: RL Policy Training in Simulation",id:"code-example-1-rl-policy-training-in-simulation",level:2},{value:"Code Example 2: Domain Randomization for Sim-to-Real Transfer",id:"code-example-2-domain-randomization-for-sim-to-real-transfer",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"reinforcement-learning-and-sim-to-real-transfer",children:"Reinforcement Learning and Sim-to-Real Transfer"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Implement reinforcement learning algorithms for robotics tasks"}),"\n",(0,r.jsx)(e.li,{children:"Design simulation environments that facilitate sim-to-real transfer"}),"\n",(0,r.jsx)(e.li,{children:"Apply domain randomization techniques to improve transferability"}),"\n",(0,r.jsx)(e.li,{children:"Evaluate and validate sim-to-real transfer performance"}),"\n",(0,r.jsx)(e.li,{children:"Understand the challenges and solutions in transferring RL policies from simulation to real robots"}),"\n",(0,r.jsx)(e.li,{children:"Design robust control policies that work in both simulation and reality"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understanding of reinforcement learning fundamentals"}),"\n",(0,r.jsx)(e.li,{children:"Experience with robotics simulation (covered in Weeks 6-9)"}),"\n",(0,r.jsx)(e.li,{children:"Knowledge of neural networks and deep learning"}),"\n",(0,r.jsx)(e.li,{children:"Familiarity with physics engines and their parameters"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,r.jsx)(e.p,{children:"Reinforcement Learning (RL) has emerged as a powerful approach for learning robot control policies. The key advantage of RL is its ability to learn complex behaviors through trial and error without explicit programming of the desired behavior. However, applying RL to real robots faces the challenge of sample efficiency and safety concerns, making simulation a critical component."}),"\n",(0,r.jsx)(e.h3,{id:"sim-to-real-transfer-challenge",children:"Sim-to-Real Transfer Challenge"}),"\n",(0,r.jsx)(e.p,{children:"The sim-to-real transfer problem arises because there are discrepancies between simulation and reality:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Dynamics Mismatch"}),": Differences in friction, inertia, and other physical parameters"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor Noise"}),": Real sensors have different noise characteristics than simulated ones"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Actuator Delays"}),": Real actuators have delays and bandwidth limitations not modeled in simulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Environmental Factors"}),": Lighting, surface textures, and other environmental conditions differ"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,r.jsx)(e.p,{children:"Domain randomization is a technique that trains policies in simulation with randomized parameters to improve robustness and transferability. By exposing the policy to a wide variety of conditions during training, it learns to adapt to variations rather than overfitting to specific simulation parameters."}),"\n",(0,r.jsx)(e.h3,{id:"domain-adaptation",children:"Domain Adaptation"}),"\n",(0,r.jsx)(e.p,{children:"Domain adaptation techniques aim to bridge the gap between simulation and reality by learning mappings between the two domains or by adjusting the policy to work in the real domain using limited real-world data."}),"\n",(0,r.jsx)(e.h2,{id:"code-example-1-rl-policy-training-in-simulation",children:"Code Example 1: RL Policy Training in Simulation"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# rl_training/ppo_trainer.py\n# Purpose: Train a PPO policy in simulation for robot navigation\n# Setup Instructions: Install stable-baselines3, gymnasium, pybullet\n# Run: python ppo_trainer.py\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\nfrom stable_baselines3.common.policies import ActorCriticPolicy\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport pybullet\nimport pybullet_envs\nimport time\n\nclass CustomCNN(BaseFeaturesExtractor):\n    """\n    Custom CNN for processing robot observations\n    """\n    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n        super().__init__(observation_space, features_dim)\n\n        # Calculate output size of conv layers\n        n_input_channels = observation_space.shape[0]\n        self.cnn = nn.Sequential(\n            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n            nn.ReLU(),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n            nn.ReLU(),\n        )\n\n        # Compute shape by doing one forward pass\n        with torch.no_grad():\n            n_flatten = self.cnn(\n                torch.as_tensor(observation_space.sample()[None]).float()\n            ).flatten().shape[0]\n\n        self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n\n    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n        return self.linear(self.cnn(observations).flatten(start_dim=1))\n\n\nclass NavigationEnv(gym.Env):\n    """\n    Custom environment for robot navigation\n    """\n    def __init__(self):\n        super(NavigationEnv, self).__init__()\n\n        # Define action and observation space\n        # Actions: [linear_velocity, angular_velocity]\n        self.action_space = spaces.Box(\n            low=np.array([-1.0, -1.0], dtype=np.float32),\n            high=np.array([1.0, 1.0], dtype=np.float32),\n            dtype=np.float32\n        )\n\n        # Observation: [x, y, theta, vel_x, vel_y, angular_vel, scan_data]\n        scan_size = 360  # 1-degree resolution LIDAR\n        obs_dim = 6 + scan_size\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n        )\n\n        # Robot state\n        self.robot_pos = np.array([0.0, 0.0])\n        self.robot_theta = 0.0\n        self.robot_vel = np.array([0.0, 0.0])\n        self.robot_ang_vel = 0.0\n\n        # Goal position\n        self.goal_pos = np.array([5.0, 5.0])\n\n        # Episode parameters\n        self.max_steps = 500\n        self.current_step = 0\n        self.reset()\n\n    def reset(self, seed=None, options=None):\n        super().reset(seed=seed)\n\n        # Reset robot to random position near origin\n        self.robot_pos = np.random.uniform(-1, 1, size=2)\n        self.robot_theta = np.random.uniform(-np.pi, np.pi)\n        self.robot_vel = np.array([0.0, 0.0])\n        self.robot_ang_vel = 0.0\n        self.current_step = 0\n\n        # Randomize goal position\n        self.goal_pos = np.random.uniform(2, 8, size=2)\n\n        observation = self._get_observation()\n        return observation, {}\n\n    def step(self, action):\n        # Unpack action\n        linear_vel_cmd = action[0]\n        angular_vel_cmd = action[1]\n\n        # Update robot state (simple kinematic model)\n        dt = 0.1  # Time step\n        self.robot_pos[0] += self.robot_vel[0] * dt\n        self.robot_pos[1] += self.robot_vel[1] * dt\n        self.robot_theta += self.robot_ang_vel * dt\n\n        # Apply velocity commands with some damping\n        self.robot_vel[0] = linear_vel_cmd * np.cos(self.robot_theta) * 0.9\n        self.robot_vel[1] = linear_vel_cmd * np.sin(self.robot_theta) * 0.9\n        self.robot_ang_vel = angular_vel_cmd * 0.9  # Damping\n\n        # Simulate LIDAR scan (simplified)\n        scan_data = self._simulate_scan()\n\n        # Calculate reward\n        reward = self._calculate_reward()\n\n        # Check termination conditions\n        terminated = self._check_termination()\n        truncated = self.current_step >= self.max_steps\n\n        self.current_step += 1\n\n        observation = self._get_observation()\n\n        return observation, reward, terminated, truncated, {}\n\n    def _get_observation(self):\n        # Robot state\n        robot_state = np.array([\n            self.robot_pos[0],\n            self.robot_pos[1],\n            self.robot_theta,\n            self.robot_vel[0],\n            self.robot_vel[1],\n            self.robot_ang_vel\n        ])\n\n        # Simulated LIDAR scan\n        scan_data = self._simulate_scan()\n\n        # Concatenate all observations\n        observation = np.concatenate([robot_state, scan_data])\n        return observation.astype(np.float32)\n\n    def _simulate_scan(self):\n        """Simulate LIDAR scan with some obstacles"""\n        scan_data = np.ones(360) * 10.0  # Max range 10m\n\n        # Add some obstacles in random positions\n        num_obstacles = np.random.randint(3, 8)\n        for _ in range(num_obstacles):\n            # Random obstacle position\n            obs_angle = np.random.uniform(0, 2*np.pi)\n            obs_dist = np.random.uniform(0.5, 4.0)\n\n            # Calculate obstacle position relative to robot\n            obs_x = self.robot_pos[0] + obs_dist * np.cos(obs_angle)\n            obs_y = self.robot_pos[1] + obs_dist * np.sin(obs_angle)\n\n            # Calculate relative position\n            rel_x = obs_x - self.robot_pos[0]\n            rel_y = obs_y - self.robot_pos[1]\n\n            # Convert to polar coordinates relative to robot heading\n            rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n            rel_angle = np.arctan2(rel_y, rel_x) - self.robot_theta\n\n            # Normalize angle to [0, 2*pi]\n            rel_angle = (rel_angle + 2*np.pi) % (2*np.pi)\n\n            # Update scan at appropriate angle (1-degree resolution)\n            angle_idx = int(np.degrees(rel_angle)) % 360\n            if rel_dist < scan_data[angle_idx]:\n                scan_data[angle_idx] = rel_dist\n\n        return scan_data\n\n    def _calculate_reward(self):\n        """Calculate reward based on robot\'s progress toward goal"""\n        # Distance to goal\n        dist_to_goal = np.linalg.norm(self.robot_pos - self.goal_pos)\n\n        # Reward based on distance reduction\n        base_reward = -dist_to_goal * 0.1  # Negative reward for distance\n\n        # Bonus for getting closer to goal\n        if hasattr(self, \'_prev_dist_to_goal\'):\n            if dist_to_goal < self._prev_dist_to_goal:\n                base_reward += 0.1  # Small bonus for progress\n        self._prev_dist_to_goal = dist_to_goal\n\n        # Large reward for reaching goal\n        if dist_to_goal < 0.5:  # Within 0.5m of goal\n            return 10.0\n\n        # Penalty for collisions (if LIDAR detects obstacle very close)\n        min_scan = np.min(self._simulate_scan())\n        if min_scan < 0.2:  # Very close to obstacle\n            return -1.0\n\n        return base_reward\n\n    def _check_termination(self):\n        """Check if episode should terminate"""\n        dist_to_goal = np.linalg.norm(self.robot_pos - self.goal_pos)\n        return dist_to_goal < 0.5  # Reached goal\n\n\ndef train_navigation_policy():\n    """Train a navigation policy using PPO"""\n\n    # Create environment\n    env = NavigationEnv()\n\n    # Create policy with custom CNN\n    policy_kwargs = dict(\n        features_extractor_class=CustomCNN,\n        features_extractor_kwargs=dict(features_dim=256),\n    )\n\n    # Create PPO agent\n    model = PPO(\n        "MultiInputPolicy",  # Actually using custom CNN policy\n        env,\n        policy_kwargs=policy_kwargs,\n        verbose=1,\n        n_steps=2048,\n        batch_size=64,\n        n_epochs=10,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        ent_coef=0.01,\n    )\n\n    print("Starting training...")\n    model.learn(total_timesteps=100000)\n\n    print("Training completed!")\n\n    # Save the model\n    model.save("navigation_policy_ppo")\n\n    return model\n\nif __name__ == "__main__":\n    trained_model = train_navigation_policy()\n    print("Model saved as navigation_policy_ppo.zip")\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Starting training...\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 123.4    |\n|    ep_rew_mean     | 2.34     |\n| time/              |          |\n|    fps             | 204      |\n|    iterations      | 1        |\n|    time_elapsed    | 10       |\n|    total_timesteps | 2048     |\n---------------------------------\n---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 156.7    |\n|    ep_rew_mean     | 3.45     |\n| time/              |          |\n|    fps             | 198      |\n|    iterations      | 2        |\n|    time_elapsed    | 20       |\n|    total_timesteps | 4096     |\n---------------------------------\n...\nTraining completed!\nModel saved as navigation_policy_ppo.zip\n"})}),"\n",(0,r.jsx)(e.h2,{id:"code-example-2-domain-randomization-for-sim-to-real-transfer",children:"Code Example 2: Domain Randomization for Sim-to-Real Transfer"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-python",children:'# sim_to_real/domain_randomization.py\n# Purpose: Implement domain randomization for sim-to-real transfer\n# Setup Instructions: Install numpy, pybullet\n# Run: python domain_randomization.py\n\nimport numpy as np\nimport random\nfrom abc import ABC, abstractmethod\n\nclass DomainRandomizationEnv:\n    """\n    Environment wrapper that applies domain randomization\n    """\n    def __init__(self, base_env, randomization_params):\n        self.base_env = base_env\n        self.randomization_params = randomization_params\n\n        # Physics parameters that will be randomized\n        self.physics_params = {\n            \'gravity\': (-15.0, -5.0),  # Gravity range\n            \'friction\': (0.1, 1.0),    # Friction coefficient\n            \'mass_multiplier\': (0.8, 1.2),  # Mass scaling\n            \'restitution\': (0.0, 0.5), # Bounce coefficient\n            \'linear_damping\': (0.0, 0.1), # Linear damping\n            \'angular_damping\': (0.0, 0.1) # Angular damping\n        }\n\n        # Sensor parameters\n        self.sensor_params = {\n            \'noise_std\': (0.0, 0.1),  # Sensor noise standard deviation\n            \'delay_range\': (0, 0.05), # Sensor delay in seconds\n        }\n\n        # Action parameters\n        self.action_params = {\n            \'actuator_noise\': (0.0, 0.05),  # Actuator noise\n            \'actuator_delay\': (0.0, 0.02), # Actuator delay\n        }\n\n        # Current randomized parameters\n        self.current_params = {}\n\n        # Randomize environment on initialization\n        self.randomize_environment()\n\n    def randomize_environment(self):\n        """Apply domain randomization to environment parameters"""\n        # Randomize physics parameters\n        for param_name, (min_val, max_val) in self.physics_params.items():\n            rand_val = np.random.uniform(min_val, max_val)\n            self.current_params[param_name] = rand_val\n\n            # Apply to PyBullet or other physics engine\n            if param_name == \'gravity\':\n                # pybullet.setGravity(0, 0, rand_val)  # Example for PyBullet\n                pass  # Placeholder for actual physics engine call\n\n        # Randomize sensor parameters\n        for param_name, (min_val, max_val) in self.sensor_params.items():\n            rand_val = np.random.uniform(min_val, max_val)\n            self.current_params[param_name] = rand_val\n\n        # Randomize action parameters\n        for param_name, (min_val, max_val) in self.action_params.items():\n            rand_val = np.random.uniform(min_val, max_val)\n            self.current_params[param_name] = rand_val\n\n    def reset(self):\n        """Reset environment with new randomization"""\n        self.randomize_environment()\n        return self.base_env.reset()\n\n    def step(self, action):\n        """Step environment with randomized parameters"""\n        # Apply action noise if specified\n        if \'actuator_noise\' in self.current_params:\n            noise_scale = self.current_params[\'actuator_noise\']\n            noisy_action = action + np.random.normal(0, noise_scale, size=action.shape)\n            # Clip to action space bounds\n            noisy_action = np.clip(noisy_action,\n                                  self.base_env.action_space.low,\n                                  self.base_env.action_space.high)\n        else:\n            noisy_action = action\n\n        # Apply actuator delay simulation\n        if \'actuator_delay\' in self.current_params:\n            delay = self.current_params[\'actuator_delay\']\n            # In a real implementation, this would buffer actions\n            pass\n\n        # Take step in base environment\n        obs, reward, terminated, truncated, info = self.base_env.step(noisy_action)\n\n        # Apply sensor noise\n        if \'noise_std\' in self.current_params:\n            noise_std = self.current_params[\'noise_std\']\n            obs = obs + np.random.normal(0, noise_std, size=obs.shape)\n\n        # Add domain parameters to info for monitoring\n        info[\'domain_params\'] = self.current_params.copy()\n\n        return obs, reward, terminated, truncated, info\n\nclass CurriculumLearning:\n    """\n    Implements curriculum learning to gradually increase difficulty\n    """\n    def __init__(self, initial_complexity=0.1, max_complexity=1.0, growth_rate=0.001):\n        self.initial_complexity = initial_complexity\n        self.max_complexity = max_complexity\n        self.growth_rate = growth_rate\n        self.current_complexity = initial_complexity\n        self.performance_history = []\n        self.episode_count = 0\n\n    def update_curriculum(self, episode_performance):\n        """Update complexity based on performance"""\n        self.performance_history.append(episode_performance)\n        self.episode_count += 1\n\n        # Keep only last 100 episodes for performance calculation\n        if len(self.performance_history) > 100:\n            self.performance_history.pop(0)\n\n        # Calculate average performance\n        if len(self.performance_history) > 10:  # Need minimum episodes\n            avg_performance = np.mean(self.performance_history[-10:])\n\n            # If performing well, increase complexity\n            if avg_performance > 0.7:  # Threshold for good performance\n                self.current_complexity = min(\n                    self.current_complexity + self.growth_rate,\n                    self.max_complexity\n                )\n            elif avg_performance < 0.3:  # Poor performance\n                self.current_complexity = max(\n                    self.current_complexity - self.growth_rate,\n                    self.initial_complexity\n                )\n\n    def get_randomization_bounds(self):\n        """Get parameter bounds based on current complexity"""\n        # Scale the randomization ranges based on curriculum\n        scaled_bounds = {}\n\n        # Example scaling for physics parameters\n        base_gravity_range = (-15.0, -5.0)\n        base_friction_range = (0.1, 1.0)\n\n        # Calculate scaled ranges\n        gravity_delta = (base_gravity_range[1] - base_gravity_range[0]) * self.current_complexity\n        friction_delta = (base_friction_range[1] - base_friction_range[0]) * self.current_complexity\n\n        scaled_bounds[\'gravity\'] = (\n            base_gravity_range[0],\n            base_gravity_range[0] + gravity_delta\n        )\n\n        scaled_bounds[\'friction\'] = (\n            base_friction_range[0],\n            base_friction_range[0] + friction_delta\n        )\n\n        return scaled_bounds\n\ndef evaluate_transfer_performance(sim_model, real_env):\n    """\n    Evaluate how well a policy trained in simulation transfers to real environment\n    """\n    print("Evaluating sim-to-real transfer...")\n\n    total_episodes = 10\n    success_count = 0\n    avg_episode_reward = 0\n\n    for episode in range(total_episodes):\n        obs, _ = real_env.reset()\n        episode_reward = 0\n        step_count = 0\n        max_steps = 500\n\n        while step_count < max_steps:\n            # Get action from trained policy\n            action, _ = sim_model.predict(obs, deterministic=True)\n\n            # Take step in real environment\n            obs, reward, terminated, truncated, info = real_env.step(action)\n            episode_reward += reward\n            step_count += 1\n\n            if terminated or truncated:\n                break\n\n        avg_episode_reward += episode_reward\n\n        # Define success criteria (e.g., reaching goal)\n        if info.get(\'success\', False):\n            success_count += 1\n\n        print(f"Episode {episode + 1}: Reward = {episode_reward:.2f}, "\n              f"Steps = {step_count}, Success = {\'Yes\' if terminated else \'No\'}")\n\n    avg_reward = avg_episode_reward / total_episodes\n    success_rate = success_count / total_episodes\n\n    print(f"\\nTransfer Performance:")\n    print(f"Average Reward: {avg_reward:.2f}")\n    print(f"Success Rate: {success_rate:.2%}")\n\n    return avg_reward, success_rate\n\n# Example usage\ndef main():\n    # This would normally connect to a real robot simulation environment\n    # For demonstration, we\'ll create a mock environment\n    class MockBaseEnv:\n        def __init__(self):\n            self.action_space = type(\'ActionSpace\', (), {\'low\': np.array([-1, -1]), \'high\': np.array([1, 1])})()\n\n        def reset(self):\n            return np.random.random(366), {}  # Mock observation\n\n        def step(self, action):\n            obs = np.random.random(366)\n            reward = np.random.random()\n            terminated = False\n            truncated = False\n            info = {}\n            return obs, reward, terminated, truncated, info\n\n    # Create base environment\n    base_env = MockBaseEnv()\n\n    # Define randomization parameters\n    randomization_params = {\n        \'physics_variations\': True,\n        \'sensor_noise\': True,\n        \'actuator_uncertainty\': True\n    }\n\n    # Create domain randomization wrapper\n    dr_env = DomainRandomizationEnv(base_env, randomization_params)\n\n    # Create curriculum learner\n    curriculum = CurriculumLearning(initial_complexity=0.1, max_complexity=0.8)\n\n    print("Domain randomization environment created")\n    print(f"Current parameters: {dr_env.current_params}")\n\n    # Example of curriculum learning update\n    for i in range(20):\n        perf = np.random.random()  # Mock performance\n        curriculum.update_curriculum(perf)\n        bounds = curriculum.get_randomization_bounds()\n        print(f"Step {i+1}: Complexity = {curriculum.current_complexity:.3f}, "\n              f"Bounds = {bounds}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{children:"Domain randomization environment created\nCurrent parameters: {'gravity': -12.34, 'friction': 0.67, 'mass_multiplier': 1.05, ...}\nStep 1: Complexity = 0.100, Bounds = {'gravity': (-15.0, -14.4), 'friction': (0.1, 0.16)}\nStep 2: Complexity = 0.101, Bounds = {'gravity': (-15.0, -14.39), 'friction': (0.1, 0.161)}\n...\nStep 20: Complexity = 0.120, Bounds = {'gravity': (-15.0, -14.2), 'friction': (0.1, 0.18)}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:"Implement a domain randomization scheme for a simulated robot arm and evaluate its effectiveness"}),"\n",(0,r.jsx)(e.li,{children:"Train a reinforcement learning policy in simulation with domain randomization and test its transfer to a slightly different simulation"}),"\n",(0,r.jsx)(e.li,{children:"Design a curriculum learning approach that gradually increases the difficulty of a manipulation task"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(e.p,{children:"Reinforcement learning provides a powerful approach for learning robot behaviors, but the sim-to-real transfer challenge remains significant. Domain randomization and curriculum learning are effective techniques for improving transferability by exposing policies to a wide range of conditions during training. The key to successful sim-to-real transfer lies in carefully designing the simulation to encompass the range of real-world variations while maintaining computational efficiency."}),"\n",(0,r.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,r.jsx)(e.p,{children:"This chapter requires specialized hardware and software:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"GPU for training reinforcement learning models"}),"\n",(0,r.jsx)(e.li,{children:"Simulation environment (PyBullet, Isaac Sim, or Gazebo)"}),"\n",(0,r.jsx)(e.li,{children:"Robot platform for real-world validation (optional for basic exercises)"}),"\n",(0,r.jsx)(e.li,{children:"Computing platform for real-time inference (NVIDIA Jetson or equivalent)"}),"\n"]})]})}function m(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}}}]);