"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[5602],{75:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"week-09/ai-powered-perception","title":"AI-Powered Perception and Manipulation","description":"Learning Objectives","source":"@site/docs/week-09/ai-powered-perception.md","sourceDirName":"week-09","slug":"/week-09/ai-powered-perception","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-09/ai-powered-perception","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-09/ai-powered-perception.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"AI-Powered Perception and Manipulation"},"sidebar":"tutorialSidebar","previous":{"title":"Isaac SDK and Isaac Sim Setup","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-08/isaac-sdk-setup"},"next":{"title":"Reinforcement Learning and Sim-to-Real Transfer","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-10/reinforcement-learning-transfer"}}');var o=t(4848),r=t(8453);const a={sidebar_position:1,title:"AI-Powered Perception and Manipulation"},s="AI-Powered Perception and Manipulation",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"AI-Based Perception",id:"ai-based-perception",level:3},{value:"AI-Based Manipulation",id:"ai-based-manipulation",level:3},{value:"Challenges and Solutions",id:"challenges-and-solutions",level:3},{value:"Code Example 1: Object Detection for Robot Perception",id:"code-example-1-object-detection-for-robot-perception",level:2},{value:"Code Example 2: AI-Based Grasp Planning",id:"code-example-2-ai-based-grasp-planning",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"ai-powered-perception-and-manipulation",children:"AI-Powered Perception and Manipulation"})}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Implement AI-based perception systems for robot applications"}),"\n",(0,o.jsx)(e.li,{children:"Train and deploy neural networks for object detection and recognition"}),"\n",(0,o.jsx)(e.li,{children:"Design manipulation strategies using AI and machine learning"}),"\n",(0,o.jsx)(e.li,{children:"Integrate perception and manipulation systems for complex robot tasks"}),"\n",(0,o.jsx)(e.li,{children:"Evaluate the performance of AI-powered robot systems"}),"\n",(0,o.jsx)(e.li,{children:"Handle uncertainty and robustness in AI-based robotics"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Understanding of basic machine learning concepts"}),"\n",(0,o.jsx)(e.li,{children:"Experience with deep learning frameworks (PyTorch, TensorFlow)"}),"\n",(0,o.jsx)(e.li,{children:"Knowledge of ROS 2 for system integration"}),"\n",(0,o.jsx)(e.li,{children:"Understanding of robot kinematics and dynamics (covered in Week 11-12)"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"theory",children:"Theory"}),"\n",(0,o.jsx)(e.p,{children:"AI-powered perception and manipulation represent a significant advancement in robotics, enabling robots to operate in unstructured environments with greater autonomy and adaptability. Traditional approaches to perception and manipulation rely on hand-crafted algorithms and predefined models, whereas AI-based approaches learn patterns and behaviors from data."}),"\n",(0,o.jsx)(e.h3,{id:"ai-based-perception",children:"AI-Based Perception"}),"\n",(0,o.jsx)(e.p,{children:"Modern perception systems leverage deep learning for tasks such as:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Object detection and classification"}),"\n",(0,o.jsx)(e.li,{children:"Semantic segmentation"}),"\n",(0,o.jsx)(e.li,{children:"Depth estimation"}),"\n",(0,o.jsx)(e.li,{children:"Pose estimation"}),"\n",(0,o.jsx)(e.li,{children:"Scene understanding"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"These systems can process various sensor modalities including RGB cameras, depth sensors, LIDAR, and IMUs to create rich understanding of the environment."}),"\n",(0,o.jsx)(e.h3,{id:"ai-based-manipulation",children:"AI-Based Manipulation"}),"\n",(0,o.jsx)(e.p,{children:"Manipulation tasks benefit from AI through:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Reinforcement learning for motor skill acquisition"}),"\n",(0,o.jsx)(e.li,{children:"Imitation learning from human demonstrations"}),"\n",(0,o.jsx)(e.li,{children:"Visual servoing guided by neural networks"}),"\n",(0,o.jsx)(e.li,{children:"Grasp planning using learned models"}),"\n",(0,o.jsx)(e.li,{children:"Adaptive control strategies"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"challenges-and-solutions",children:"Challenges and Solutions"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Uncertainty Handling"}),": AI systems must deal with uncertainty in perception and action. Techniques like Bayesian neural networks, ensemble methods, and uncertainty quantification help manage this challenge."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Real-Time Performance"}),": Many AI algorithms require significant computational resources. Optimization techniques include model compression, quantization, and specialized hardware acceleration."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Generalization"}),": AI systems trained in simulation or specific environments must generalize to new situations. Domain adaptation, sim-to-real transfer, and data augmentation techniques address this challenge."]}),"\n",(0,o.jsx)(e.h2,{id:"code-example-1-object-detection-for-robot-perception",children:"Code Example 1: Object Detection for Robot Perception"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# perception/object_detector.py\n# Purpose: Implement real-time object detection for robot perception\n# Setup Instructions: Install torch, torchvision, opencv-python\n# Run: python object_detector.py\n\nimport torch\nimport torchvision.transforms as transforms\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.utils import draw_bounding_boxes\nimport rospy\nfrom sensor_msgs.msg import Image as ImageMsg\nfrom cv_bridge import CvBridge\n\nclass ObjectDetector:\n    def __init__(self, confidence_threshold=0.5):\n        # Load pre-trained model\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n        self.model.eval()\n\n        # Confidence threshold\n        self.confidence_threshold = confidence_threshold\n\n        # COCO dataset class names\n        self.coco_names = [\n            '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n            'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n            'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n            'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n            'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n            'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n            'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n            'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n            'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n            'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n            'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n            'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n        ]\n\n        # Image transformation\n        self.transform = transforms.Compose([\n            transforms.ToTensor(),\n        ])\n\n        # Initialize ROS components\n        self.bridge = CvBridge()\n        self.object_pub = rospy.Publisher('/detected_objects', ImageMsg, queue_size=10)\n\n    def detect_objects(self, image):\n        \"\"\"\n        Detect objects in an image\n        Args:\n            image: PIL Image or numpy array\n        Returns:\n            dict: Detection results\n        \"\"\"\n        # Convert to PIL if needed\n        if isinstance(image, np.ndarray):\n            image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n        else:\n            image_pil = image\n\n        # Transform image\n        input_tensor = self.transform(image_pil).unsqueeze(0)\n\n        # Perform inference\n        with torch.no_grad():\n            predictions = self.model(input_tensor)\n\n        # Filter predictions by confidence\n        pred = predictions[0]\n        scores = pred['scores']\n        boxes = pred['boxes']\n        labels = pred['labels']\n\n        # Filter by confidence threshold\n        keep_idx = scores > self.confidence_threshold\n        filtered_boxes = boxes[keep_idx]\n        filtered_labels = labels[keep_idx]\n        filtered_scores = scores[keep_idx]\n\n        # Convert to numpy arrays\n        boxes_np = filtered_boxes.numpy()\n        labels_np = filtered_labels.numpy()\n        scores_np = filtered_scores.numpy()\n\n        # Create results dictionary\n        results = {\n            'boxes': boxes_np,\n            'labels': [self.coco_names[label] for label in labels_np],\n            'scores': scores_np,\n            'num_detections': len(boxes_np)\n        }\n\n        return results\n\n    def annotate_image(self, image, results):\n        \"\"\"Draw bounding boxes on image\"\"\"\n        # Convert image to tensor for drawing\n        img_tensor = torch.from_numpy(image).permute(2, 0, 1).byte()  # CHW format\n\n        # Convert boxes to integer format\n        boxes = torch.tensor(results['boxes']).int()\n\n        # Create labels for drawing\n        labels = [f\"{label}: {score:.2f}\" for label, score in zip(results['labels'], results['scores'])]\n\n        # Draw bounding boxes\n        annotated_img = draw_bounding_boxes(\n            image=img_tensor,\n            boxes=boxes,\n            labels=labels,\n            colors=['red'] * len(boxes),\n            width=2\n        )\n\n        # Convert back to numpy\n        annotated_img_np = annotated_img.permute(1, 2, 0).numpy()\n\n        return annotated_img_np\n\n# Example usage\ndef main():\n    rospy.init_node('object_detector_node', anonymous=True)\n\n    detector = ObjectDetector(confidence_threshold=0.7)\n\n    # Example: Process a single image\n    # In practice, this would subscribe to camera topic\n    cap = cv2.VideoCapture(0)  # Use webcam\n\n    try:\n        while not rospy.is_shutdown():\n            ret, frame = cap.read()\n            if not ret:\n                continue\n\n            # Detect objects\n            results = detector.detect_objects(frame)\n\n            # Annotate image\n            annotated_frame = detector.annotate_image(frame, results)\n\n            # Display results\n            cv2.imshow('Object Detection', annotated_frame)\n\n            # Print detection results\n            if results['num_detections'] > 0:\n                print(f\"Detected {results['num_detections']} objects:\")\n                for i in range(results['num_detections']):\n                    print(f\"  - {results['labels'][i]}: {results['scores'][i]:.2f}\")\n            else:\n                print(\"No objects detected\")\n\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n    except KeyboardInterrupt:\n        print(\"Shutting down...\")\n    finally:\n        cap.release()\n        cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Detected 3 objects:\n  - person: 0.98\n  - chair: 0.87\n  - laptop: 0.76\nDetected 2 objects:\n  - person: 0.99\n  - cup: 0.81\nNo objects detected\n"})}),"\n",(0,o.jsx)(e.h2,{id:"code-example-2-ai-based-grasp-planning",children:"Code Example 2: AI-Based Grasp Planning"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# manipulation/grasp_planner.py\n# Purpose: Implement AI-based grasp planning for robot manipulation\n# Setup Instructions: Install numpy, scipy, robotiq_gripper_control\n# Run: python grasp_planner.py\n\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport math\nfrom collections import deque\n\nclass GraspPlanner:\n    def __init__(self, gripper_width_range=(0.01, 0.08), finger_length=0.05):\n        """\n        Initialize grasp planner\n        Args:\n            gripper_width_range: Tuple of (min, max) gripper width in meters\n            finger_length: Length of gripper fingers in meters\n        """\n        self.min_width = gripper_width_range[0]\n        self.max_width = gripper_width_range[1]\n        self.finger_length = finger_length\n\n        # Grasp candidates storage\n        self.grasp_candidates = []\n\n    def plan_grasps_from_point_cloud(self, point_cloud, object_center=None):\n        """\n        Plan grasps from point cloud data\n        Args:\n            point_cloud: Nx3 numpy array of 3D points\n            object_center: Center of object to grasp (optional)\n        Returns:\n            list: List of feasible grasp poses\n        """\n        if object_center is None:\n            # Estimate object center as centroid of point cloud\n            object_center = np.mean(point_cloud, axis=0)\n\n        # Find surface points on object\n        surface_points = self._extract_surface_points(point_cloud)\n\n        # Generate grasp candidates\n        grasp_poses = self._generate_grasp_candidates(surface_points, object_center)\n\n        # Filter feasible grasps\n        feasible_grasps = self._filter_feasible_grasps(grasp_poses, point_cloud)\n\n        return feasible_grasps\n\n    def _extract_surface_points(self, point_cloud, neighborhood_radius=0.02):\n        """Extract surface points using normal estimation"""\n        surface_points = []\n\n        # For each point, check if it\'s on a surface\n        for i, point in enumerate(point_cloud):\n            # Find neighboring points\n            distances = cdist([point], point_cloud)[0]\n            neighbors = point_cloud[distances < neighborhood_radius]\n\n            if len(neighbors) >= 3:\n                # Estimate surface normal\n                cov_matrix = np.cov(neighbors.T)\n                eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n                # Normal is the eigenvector corresponding to smallest eigenvalue\n                normal = eigenvectors[:, 0]\n\n                # Check if this is a surface point (low variation in normal direction)\n                if eigenvalues[0] < 0.001:  # Surface point\n                    surface_points.append({\n                        \'position\': point,\n                        \'normal\': normal\n                    })\n\n        return surface_points\n\n    def _generate_grasp_candidates(self, surface_points, object_center):\n        """Generate potential grasp poses"""\n        grasp_candidates = []\n\n        for i, point_data in enumerate(surface_points):\n            pos = point_data[\'position\']\n            normal = point_data[\'normal\']\n\n            # Generate multiple grasp orientations around the surface normal\n            for angle_offset in np.linspace(0, 2*np.pi, 8):  # 8 orientations per point\n                # Create grasp orientation\n                grasp_orientation = self._compute_grasp_orientation(normal, angle_offset)\n\n                # Create grasp pose\n                grasp_pose = {\n                    \'position\': pos,\n                    \'orientation\': grasp_orientation,\n                    \'approach_direction\': -normal  # Approach opposite to surface normal\n                }\n\n                grasp_candidates.append(grasp_pose)\n\n        return grasp_candidates\n\n    def _compute_grasp_orientation(self, surface_normal, angle_offset):\n        """Compute grasp orientation based on surface normal"""\n        # Find two orthogonal vectors to the surface normal\n        # First, find a vector not parallel to the normal\n        if abs(surface_normal[2]) < 0.9:\n            temp_vec = np.array([0, 0, 1])\n        else:\n            temp_vec = np.array([1, 0, 0])\n\n        # Create two orthogonal vectors\n        ortho1 = np.cross(surface_normal, temp_vec)\n        ortho1 = ortho1 / np.linalg.norm(ortho1)\n        ortho2 = np.cross(surface_normal, ortho1)\n        ortho2 = ortho2 / np.linalg.norm(ortho2)\n\n        # Rotate ortho1 and ortho2 by angle_offset around the normal\n        rotated_ortho1 = (\n            ortho1 * np.cos(angle_offset) +\n            ortho2 * np.sin(angle_offset)\n        )\n        rotated_ortho2 = (\n            -ortho1 * np.sin(angle_offset) +\n            ortho2 * np.cos(angle_offset)\n        )\n\n        # Create rotation matrix\n        # Columns are: approach (normal), binormal1, binormal2\n        grasp_orientation = np.column_stack([\n            -surface_normal,  # Approach direction (into surface)\n            rotated_ortho1,   # Gripper opening direction\n            rotated_ortho2    # Gripper up direction\n        ])\n\n        return grasp_orientation\n\n    def _filter_feasible_grasps(self, grasp_poses, point_cloud, min_grasp_width=0.02):\n        """Filter grasp poses based on geometric feasibility"""\n        feasible_grasps = []\n\n        for grasp in grasp_poses:\n            position = grasp[\'position\']\n            orientation = grasp[\'orientation\']\n\n            # Check if grasp width is feasible\n            grasp_width = self._estimate_object_width_at_grasp(position, orientation, point_cloud)\n\n            if self.min_width <= grasp_width <= self.max_width:\n                # Check if there are sufficient contact points for a stable grasp\n                contact_points = self._check_contact_points(position, orientation, point_cloud)\n\n                if contact_points >= 2:  # At least 2 contact points needed\n                    grasp[\'estimated_width\'] = grasp_width\n                    grasp[\'contact_points\'] = contact_points\n                    feasible_grasps.append(grasp)\n\n        # Sort grasps by estimated quality\n        feasible_grasps.sort(key=lambda g: g[\'contact_points\'], reverse=True)\n\n        return feasible_grasps\n\n    def _estimate_object_width_at_grasp(self, position, orientation, point_cloud):\n        """Estimate object width at a potential grasp location"""\n        # Project points onto the gripper plane\n        gripper_normal = orientation[:, 0]  # First column is approach direction\n        gripper_binormal = orientation[:, 1]  # Second column is gripper opening direction\n\n        # Find points near the grasp position\n        distances = cdist([position], point_cloud)[0]\n        nearby_points = point_cloud[distances < 0.05]  # 5cm radius\n\n        if len(nearby_points) == 0:\n            return 0.0\n\n        # Project points onto gripper opening direction\n        projections = np.dot(nearby_points - position, gripper_binormal)\n\n        # Width is the range of projections\n        width = np.max(projections) - np.min(projections)\n\n        return width\n\n    def _check_contact_points(self, position, orientation, point_cloud):\n        """Check if there are sufficient contact points for a stable grasp"""\n        gripper_normal = orientation[:, 0]\n        gripper_binormal = orientation[:, 1]\n\n        # Define contact regions on either side of the gripper\n        gripper_width = self.min_width  # Use minimum width for conservative estimate\n        left_contact_region = position + (gripper_width/2) * gripper_binormal\n        right_contact_region = position - (gripper_width/2) * gripper_binormal\n\n        # Count points in contact regions\n        left_distances = cdist([left_contact_region], point_cloud)[0]\n        right_distances = cdist([right_contact_region], point_cloud)[0]\n\n        left_contacts = np.sum(left_distances < 0.01)  # 1cm contact region\n        right_contacts = np.sum(right_distances < 0.01)\n\n        return min(left_contacts, right_contacts)  # Both sides need contacts\n\n# Example usage\ndef main():\n    # Simulate a point cloud of an object (e.g., a cube)\n    np.random.seed(42)\n\n    # Create a simple cube point cloud\n    cube_points = []\n    for x in np.linspace(-0.05, 0.05, 10):\n        for y in np.linspace(-0.03, 0.03, 6):\n            for z in np.linspace(-0.02, 0.02, 4):\n                # Add some noise\n                point = np.array([x, y, z]) + np.random.normal(0, 0.001, 3)\n                cube_points.append(point)\n\n    point_cloud = np.array(cube_points)\n\n    # Create grasp planner\n    planner = GraspPlanner()\n\n    # Plan grasps\n    feasible_grasps = planner.plan_grasps_from_point_cloud(point_cloud)\n\n    print(f"Found {len(feasible_grasps)} feasible grasps")\n\n    if feasible_grasps:\n        best_grasp = feasible_grasps[0]\n        print(f"Best grasp at position: {best_grasp[\'position\']}")\n        print(f"Estimated object width: {best_grasp[\'estimated_width\']:.3f}m")\n        print(f"Contact points: {best_grasp[\'contact_points\']}")\n        print(f"Grasp orientation:\\n{best_grasp[\'orientation\']}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Expected Output:"})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Found 24 feasible grasps\nBest grasp at position: [-0.05 -0.03 -0.02]\nEstimated object width: 0.098m\nContact points: 5\nGrasp orientation:\n[[ 0.000 -1.000  0.000]\n [ 1.000  0.000  0.000]\n [ 0.000  0.000  1.000]]\n"})}),"\n",(0,o.jsx)(e.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Implement a CNN-based classifier to recognize specific objects in your robot's environment"}),"\n",(0,o.jsx)(e.li,{children:"Train a reinforcement learning agent to perform simple manipulation tasks in simulation"}),"\n",(0,o.jsx)(e.li,{children:"Develop a perception pipeline that integrates multiple sensor modalities (camera, LIDAR) for improved object detection"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"AI-powered perception and manipulation enable robots to operate in complex, unstructured environments with greater autonomy. By leveraging deep learning and machine learning techniques, robots can recognize objects, understand scenes, and plan manipulation actions that would be difficult to encode with traditional programming approaches. The integration of AI with robotics requires careful consideration of real-time performance, uncertainty handling, and safety."}),"\n",(0,o.jsx)(e.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,o.jsx)(e.p,{children:"This chapter requires specialized hardware and software:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"GPU with CUDA support for neural network inference"}),"\n",(0,o.jsx)(e.li,{children:"Robot platform with manipulation capabilities"}),"\n",(0,o.jsx)(e.li,{children:"RGB-D camera or LIDAR for perception"}),"\n",(0,o.jsx)(e.li,{children:"Simulation environment for training (Isaac Sim, Gazebo, or PyBullet)"}),"\n",(0,o.jsx)(e.li,{children:"Computing platform for real-time inference (NVIDIA Jetson or equivalent)"}),"\n"]})]})}function d(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>s});var i=t(6540);const o={},r=i.createContext(o);function a(n){const e=i.useContext(r);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),i.createElement(r.Provider,{value:e},n.children)}}}]);