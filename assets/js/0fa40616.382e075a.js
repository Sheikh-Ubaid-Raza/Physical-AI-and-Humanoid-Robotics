"use strict";(globalThis.webpackChunkwebsite_name=globalThis.webpackChunkwebsite_name||[]).push([[7041],{1246:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"week-13/conversational-robotics","title":"Conversational Robotics: GPT Integration for Conversational AI","description":"Learning Objectives","source":"@site/docs/week-13/conversational-robotics.md","sourceDirName":"week-13","slug":"/week-13/conversational-robotics","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-13/conversational-robotics","draft":false,"unlisted":false,"editUrl":"https://github.com/Sheikh-Ubaid-Raza/Physical-AI-and-Humanoid-Robotics/edit/main/docs/week-13/conversational-robotics.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Conversational Robotics: GPT Integration for Conversational AI"},"sidebar":"tutorialSidebar","previous":{"title":"Manipulation, Grasping, Natural Human-Robot Interaction","permalink":"/Physical-AI-and-Humanoid-Robotics/docs/week-12/manipulation-grasping"}}');var i=o(4848),s=o(8453);const r={sidebar_position:1,title:"Conversational Robotics: GPT Integration for Conversational AI"},a="Conversational Robotics: GPT Integration for Conversational AI",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Theory",id:"theory",level:2},{value:"Key Components of Conversational Robotics",id:"key-components-of-conversational-robotics",level:3},{value:"Integration Approaches",id:"integration-approaches",level:3},{value:"Challenges in Conversational Robotics",id:"challenges-in-conversational-robotics",level:3},{value:"Code Example 1: GPT Integration for Robot Command Interpretation",id:"code-example-1-gpt-integration-for-robot-command-interpretation",level:2},{value:"Code Example 2: Multimodal Dialogue System",id:"code-example-2-multimodal-dialogue-system",level:2},{value:"Hands-on Exercises",id:"hands-on-exercises",level:2},{value:"Summary",id:"summary",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"conversational-robotics-gpt-integration-for-conversational-ai",children:"Conversational Robotics: GPT Integration for Conversational AI"})}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate large language models (LLMs) like GPT into robotic systems"}),"\n",(0,i.jsx)(n.li,{children:"Implement speech recognition and natural language understanding for robots"}),"\n",(0,i.jsx)(n.li,{children:"Design conversation flows for natural human-robot interaction"}),"\n",(0,i.jsx)(n.li,{children:"Process and interpret natural language commands for robot control"}),"\n",(0,i.jsx)(n.li,{children:"Implement multimodal dialogue systems combining speech, vision, and action"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate and improve conversational robot performance"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding of natural human-robot interaction (covered in Week 12)"}),"\n",(0,i.jsx)(n.li,{children:"Experience with ROS 2 for system integration"}),"\n",(0,i.jsx)(n.li,{children:"Basic knowledge of natural language processing"}),"\n",(0,i.jsx)(n.li,{children:"Understanding of speech recognition and synthesis"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"theory",children:"Theory"}),"\n",(0,i.jsx)(n.p,{children:"Conversational robotics combines natural language processing, speech recognition, and robotic control to create robots that can engage in natural conversations with humans. This enables more intuitive and accessible human-robot interaction, moving beyond button pressing or gesture-based interfaces to natural language communication."}),"\n",(0,i.jsx)(n.h3,{id:"key-components-of-conversational-robotics",children:"Key Components of Conversational Robotics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Automatic Speech Recognition (ASR)"}),": Converts human speech to text for processing by the language model."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Understanding (NLU)"}),": Interprets the meaning of user utterances and extracts relevant information."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Dialog Management"}),": Maintains conversation state and determines appropriate robot responses."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Natural Language Generation (NLG)"}),": Creates natural-sounding responses from the robot."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Text-to-Speech (TTS)"}),": Converts robot responses back to spoken language."]}),"\n",(0,i.jsx)(n.h3,{id:"integration-approaches",children:"Integration Approaches"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Cloud-based LLMs"}),": Services like OpenAI's GPT provide powerful language understanding but require network connectivity."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"On-device Models"}),": Smaller language models that can run locally for improved privacy and responsiveness."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hybrid Approaches"}),": Combining cloud and local processing for optimal performance."]}),"\n",(0,i.jsx)(n.h3,{id:"challenges-in-conversational-robotics",children:"Challenges in Conversational Robotics"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ambiguity Resolution"}),": Natural language often contains ambiguous references that require context to resolve."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Grounding"}),": Connecting linguistic expressions to the physical world and robot capabilities."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multimodal Integration"}),": Coordinating speech, vision, and action for coherent interaction."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real-time Processing"}),": Meeting real-time constraints for responsive interaction."]}),"\n",(0,i.jsx)(n.h2,{id:"code-example-1-gpt-integration-for-robot-command-interpretation",children:"Code Example 1: GPT Integration for Robot Command Interpretation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# conversational_ai/gpt_robot_interface.py\n# Purpose: Integrate GPT with robot command interpretation\n# Setup Instructions: Install openai, speechrecognition, pyttsx3, transformers\n# Run: python gpt_robot_interface.py\n\nimport openai\nimport speech_recognition as sr\nimport pyttsx3\nimport json\nimport re\nimport time\nfrom typing import Dict, List, Optional, Tuple\nimport asyncio\nimport threading\n\nclass GPTRobotInterface:\n    """\n    Interface between GPT language model and robot control system\n    """\n    def __init__(self, api_key: str, model_name: str = "gpt-3.5-turbo"):\n        # Initialize OpenAI client\n        openai.api_key = api_key\n        self.model_name = model_name\n\n        # Initialize speech components\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Initialize text-to-speech\n        self.tts_engine = pyttsx3.init()\n        voices = self.tts_engine.getProperty(\'voices\')\n        if voices:\n            self.tts_engine.setProperty(\'voice\', voices[0].id)\n        self.tts_engine.setProperty(\'rate\', 150)  # Words per minute\n\n        # Robot capabilities and environment context\n        self.robot_capabilities = [\n            "move forward", "move backward", "turn left", "turn right",\n            "move to location", "pick up object", "put down object",\n            "look around", "take photo", "stop", "introduce yourself",\n            "tell me about yourself", "help", "what can you do"\n        ]\n\n        self.environment_context = {\n            "locations": ["kitchen", "living room", "bedroom", "office", "hallway"],\n            "objects": ["cup", "book", "phone", "keys", "water bottle", "snack"],\n            "robot_name": "Robbie",\n            "robot_purpose": "household assistant"\n        }\n\n        # Conversation history\n        self.conversation_history = []\n\n    def set_microphone_parameters(self):\n        """Adjust microphone sensitivity"""\n        with self.microphone as source:\n            print("Adjusting microphone for ambient noise...")\n            self.recognizer.adjust_for_ambient_noise(source, duration=1)\n\n    def listen_for_speech(self, timeout: int = 5) -> Optional[str]:\n        """\n        Listen for speech and convert to text\n        """\n        try:\n            with self.microphone as source:\n                print("Listening...")\n                audio = self.recognizer.listen(source, timeout=timeout)\n\n                print("Processing speech...")\n                text = self.recognizer.recognize_google(audio)\n                print(f"Heard: {text}")\n                return text\n\n        except sr.WaitTimeoutError:\n            print("No speech detected within timeout period")\n            return None\n        except sr.UnknownValueError:\n            print("Could not understand audio")\n            return None\n        except sr.RequestError as e:\n            print(f"Speech recognition error: {e}")\n            return None\n\n    def speak_text(self, text: str):\n        """Convert text to speech"""\n        print(f"Speaking: {text}")\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n\n    def generate_robot_response(self, user_input: str) -> Tuple[str, Optional[Dict]]:\n        """\n        Generate robot response using GPT and extract robot command\n        """\n        # Prepare context for GPT\n        system_prompt = f"""\n        You are {self.environment_context[\'robot_name\']}, a {self.environment_context[\'robot_purpose\']}.\n        Your capabilities include: {\', \'.join(self.robot_capabilities)}.\n        Locations in the environment include: {\', \'.join(self.environment_context[\'locations\'])}.\n        Objects you can interact with include: {\', \'.join(self.environment_context[\'objects\'])}.\n\n        Respond naturally to the user. If the user gives a command that matches your capabilities,\n        provide a natural response and include the command in JSON format.\n\n        If the user asks you to do something, respond with a JSON object containing:\n        {{\n            "command": "...",\n            "action": "...",\n            "parameters": {{}},\n            "explanation": "..."\n        }}\n\n        For example:\n        User: "Can you go to the kitchen?"\n        Response: "Sure, I\'ll go to the kitchen." and JSON: {{"command": "navigate", "action": "move_to_location", "parameters": {{"location": "kitchen"}}, "explanation": "Navigating to kitchen"}}\n\n        If you cannot fulfill a request, politely explain why.\n        Keep responses concise and natural.\n        """\n\n        # Add conversation history to maintain context\n        messages = [{"role": "system", "content": system_prompt}]\n\n        # Add conversation history (last 5 exchanges to manage token usage)\n        for msg in self.conversation_history[-5:]:\n            messages.append({"role": msg["role"], "content": msg["content"]})\n\n        # Add current user input\n        messages.append({"role": "user", "content": user_input})\n\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model_name,\n                messages=messages,\n                temperature=0.7,\n                max_tokens=200\n            )\n\n            full_response = response.choices[0].message.content.strip()\n\n            # Extract command if present in response\n            command_json = self.extract_command_from_response(full_response)\n\n            # Add to conversation history\n            self.conversation_history.append({"role": "user", "content": user_input})\n            self.conversation_history.append({"role": "assistant", "content": full_response})\n\n            return full_response, command_json\n\n        except Exception as e:\n            print(f"Error calling GPT: {e}")\n            fallback_response = "I\'m sorry, I\'m having trouble processing that request right now."\n            return fallback_response, None\n\n    def extract_command_from_response(self, response: str) -> Optional[Dict]:\n        """\n        Extract command from GPT response using regex\n        """\n        # Look for JSON-like structure in response\n        json_match = re.search(r\'\\{.*\\}\', response, re.DOTALL)\n        if json_match:\n            try:\n                json_str = json_match.group()\n                command = json.loads(json_str)\n                return command\n            except json.JSONDecodeError:\n                pass\n\n        # If no JSON found, try to infer simple commands\n        user_input_lower = response.lower()\n\n        if any(word in user_input_lower for word in ["move", "go", "navigate", "walk"]):\n            for loc in self.environment_context[\'locations\']:\n                if loc in user_input_lower:\n                    return {\n                        "command": "navigate",\n                        "action": "move_to_location",\n                        "parameters": {"location": loc},\n                        "explanation": f"Moving to {loc}"\n                    }\n\n        return None\n\n    def execute_robot_command(self, command: Dict) -> bool:\n        """\n        Execute robot command (simulated)\n        """\n        action = command.get(\'action\', \'\')\n        params = command.get(\'parameters\', {})\n\n        print(f"Executing command: {action} with parameters: {params}")\n\n        # Simulate command execution\n        if action == "move_to_location":\n            location = params.get(\'location\', \'unknown\')\n            print(f"Robot is moving to {location}...")\n            # Simulate movement time\n            time.sleep(2)\n            print(f"Robot has arrived at {location}")\n            return True\n\n        elif action == "pick_up_object":\n            obj = params.get(\'object\', \'unknown\')\n            print(f"Robot is picking up {obj}...")\n            time.sleep(1)\n            print(f"Robot has picked up {obj}")\n            return True\n\n        elif action == "put_down_object":\n            print("Robot is putting down object...")\n            time.sleep(1)\n            print("Robot has put down object")\n            return True\n\n        elif action == "navigate":\n            location = params.get(\'location\', \'unknown\')\n            print(f"Robot is navigating to {location}...")\n            time.sleep(2)\n            print(f"Robot has navigated to {location}")\n            return True\n\n        else:\n            print(f"Unknown command: {action}")\n            return False\n\n    def process_user_interaction(self) -> bool:\n        """\n        Process a complete user interaction cycle\n        """\n        # Listen for user input\n        user_input = self.listen_for_speech()\n        if not user_input:\n            return False\n\n        # Generate response using GPT\n        response, command = self.generate_robot_response(user_input)\n\n        # Speak the response\n        self.speak_text(response)\n\n        # Execute command if available\n        if command:\n            success = self.execute_robot_command(command)\n            if success:\n                confirmation = f"I have completed the task: {command.get(\'explanation\', \'\')}"\n                self.speak_text(confirmation)\n            else:\n                error_msg = "I had trouble executing that command."\n                self.speak_text(error_msg)\n\n        return True\n\n# Example usage\ndef main():\n    # This would normally be your OpenAI API key\n    # api_key = "your-openai-api-key-here"\n    api_key = "fake-api-key-for-demo"  # Placeholder for demonstration\n\n    try:\n        # Initialize the GPT robot interface\n        print("Initializing GPT Robot Interface...")\n        robot_interface = GPTRobotInterface(api_key)\n\n        print("Setting up microphone...")\n        robot_interface.set_microphone_parameters()\n\n        print("GPT Robot Interface ready!")\n        print("Say something to interact with the robot.")\n        print("Example commands: \'Go to the kitchen\', \'Pick up the cup\', \'Tell me about yourself\'")\n        print("Press Ctrl+C to exit")\n\n        # Main interaction loop\n        interaction_count = 0\n        while True:\n            print(f"\\n--- Interaction {interaction_count + 1} ---")\n\n            # Simulate user input for demonstration\n            # In real implementation, this would listen for actual speech\n            demo_inputs = [\n                "Hi Robbie, can you go to the kitchen?",\n                "Can you pick up the water bottle?",\n                "Tell me about yourself",\n                "What can you do?",\n                "Take a photo"\n            ]\n\n            if interaction_count < len(demo_inputs):\n                user_input = demo_inputs[interaction_count]\n                print(f"(Demo) Heard: {user_input}")\n\n                # Generate response\n                response, command = robot_interface.generate_robot_response(user_input)\n\n                print(f"Robot response: {response}")\n                if command:\n                    print(f"Extracted command: {command}")\n\n                    # Execute command\n                    if command.get(\'action\'):\n                        success = robot_interface.execute_robot_command(command)\n                        if success:\n                            print(f"Command executed successfully: {command.get(\'explanation\', \'\')}")\n                else:\n                    print("No command extracted from response")\n\n                interaction_count += 1\n            else:\n                break  # Exit after demo inputs\n\n            time.sleep(1)  # Pause between interactions\n\n    except KeyboardInterrupt:\n        print("\\nShutting down GPT Robot Interface...")\n    except Exception as e:\n        print(f"Error initializing GPT Robot Interface: {e}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Initializing GPT Robot Interface...\nSetting up microphone...\nAdjusting microphone for ambient noise...\nGPT Robot Interface ready!\nSay something to interact with the robot.\nExample commands: 'Go to the kitchen', 'Pick up the cup', 'Tell me about yourself'\n\n--- Interaction 1 ---\n(Demo) Heard: Hi Robbie, can you go to the kitchen?\nRobot response: Hello! I'm Robbie, your household assistant. I'd be happy to go to the kitchen for you.\nNo command extracted from response\n\n--- Interaction 2 ---\n(Demo) Heard: Can you pick up the water bottle?\nRobot response: I'll pick up the water bottle for you.\nNo command extracted from response\n\n--- Interaction 3 ---\n(Demo) Heard: Tell me about yourself\nRobot response: Hi there! I'm Robbie, your household assistant robot. I can help with various tasks around the house like navigating to different rooms, picking up objects, taking photos, and more. How can I assist you today?\nNo command extracted from response\n"})}),"\n",(0,i.jsx)(n.h2,{id:"code-example-2-multimodal-dialogue-system",children:"Code Example 2: Multimodal Dialogue System"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# conversational_ai/multimodal_dialogue.py\n# Purpose: Implement multimodal dialogue system combining speech, vision, and action\n# Setup Instructions: Install openai, speechrecognition, pyttsx3, opencv-python\n# Run: python multimodal_dialogue.py\n\nimport cv2\nimport numpy as np\nimport speech_recognition as sr\nimport pyttsx3\nimport json\nimport threading\nimport time\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple\nimport queue\n\n@dataclass\nclass VisualObservation:\n    objects: List[Dict]  # List of detected objects with properties\n    scene_description: str\n    timestamp: float\n\nclass MultimodalDialogueSystem:\n    """\n    Multimodal dialogue system combining speech, vision, and action\n    """\n    def __init__(self, gpt_api_key: str):\n        # Initialize speech components\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n        self.tts_engine = pyttsx3.init()\n\n        # Initialize camera\n        self.camera = cv2.VideoCapture(0)\n        if not self.camera.isOpened():\n            print("Warning: Could not open camera")\n            self.camera = None\n\n        # Initialize queues for asynchronous processing\n        self.speech_queue = queue.Queue()\n        self.vision_queue = queue.Queue()\n        self.response_queue = queue.Queue()\n\n        # System state\n        self.current_objects = []\n        self.conversation_context = {\n            "last_seen_objects": [],\n            "current_location": "unknown",\n            "task_in_progress": None\n        }\n\n        # GPT configuration\n        self.gpt_api_key = gpt_api_key\n        self.gpt_model = "gpt-3.5-turbo"\n\n    def start_listening_async(self):\n        """Start listening for speech in a separate thread"""\n        def listen_loop():\n            with self.microphone as source:\n                self.recognizer.adjust_for_ambient_noise(source, duration=1)\n\n            while True:\n                try:\n                    with self.microphone as source:\n                        print("Listening for speech...")\n                        audio = self.recognizer.listen(source, timeout=5)\n\n                    text = self.recognizer.recognize_google(audio)\n                    print(f"Heard: {text}")\n\n                    # Add to queue for processing\n                    self.speech_queue.put({\n                        "type": "speech",\n                        "text": text,\n                        "timestamp": time.time()\n                    })\n\n                except sr.WaitTimeoutError:\n                    continue\n                except sr.UnknownValueError:\n                    print("Could not understand audio")\n                except sr.RequestError as e:\n                    print(f"Speech recognition error: {e}")\n                    time.sleep(1)\n\n        # Start listening thread\n        listen_thread = threading.Thread(target=listen_loop, daemon=True)\n        listen_thread.start()\n\n    def capture_visual_observations(self) -> Optional[VisualObservation]:\n        """Capture and process visual information from camera"""\n        if not self.camera:\n            return None\n\n        ret, frame = self.camera.read()\n        if not ret:\n            return None\n\n        # Simple object detection simulation\n        # In real implementation, this would use a computer vision model\n        detected_objects = self._simulate_object_detection(frame)\n\n        # Create scene description\n        scene_desc = self._describe_scene(detected_objects)\n\n        return VisualObservation(\n            objects=detected_objects,\n            scene_description=scene_desc,\n            timestamp=time.time()\n        )\n\n    def _simulate_object_detection(self, frame) -> List[Dict]:\n        """Simulate object detection (in real system, use actual CV model)"""\n        # For simulation, return some common objects\n        if np.random.random() > 0.3:  # 70% chance of detecting objects\n            objects = [\n                {"name": "cup", "confidence": 0.89, "bbox": [100, 150, 150, 200]},\n                {"name": "book", "confidence": 0.76, "bbox": [200, 100, 300, 250]},\n                {"name": "phone", "confidence": 0.92, "bbox": [350, 200, 400, 300]}\n            ]\n            return objects\n\n        return []\n\n    def _describe_scene(self, objects: List[Dict]) -> str:\n        """Generate a textual description of the current scene"""\n        if not objects:\n            return "I don\'t see any specific objects in front of me right now."\n\n        object_names = [obj["name"] for obj in objects]\n        object_str = ", ".join(object_names)\n\n        return f"I can see the following objects: {object_str}. The scene appears to be well-lit."\n\n    def process_multimodal_input(self, speech_input: str, visual_obs: VisualObservation) -> str:\n        """\n        Process speech input in the context of visual observations\n        """\n        # Prepare context for GPT\n        context = f"""\n        You are a helpful robot assistant. Here\'s what you\'re seeing:\n        {visual_obs.scene_description}\n\n        The user just said: "{speech_input}"\n\n        Based on what you see and what the user said, respond appropriately.\n        If the user is asking about objects you can see, reference them specifically.\n        If the user wants you to do something with visible objects, acknowledge what you see.\n        Keep your response natural and helpful.\n        """\n\n        # In a real implementation, this would call the GPT API\n        # For this example, we\'ll simulate a response\n        response = self._simulate_gpt_response(speech_input, visual_obs)\n\n        return response\n\n    def _simulate_gpt_response(self, speech_input: str, visual_obs: VisualObservation) -> str:\n        """Simulate GPT response for demonstration"""\n        speech_lower = speech_input.lower()\n\n        # Check if user is asking about visible objects\n        if any(obj["name"] in speech_lower for obj in visual_obs.objects):\n            visible_obj = next((obj for obj in visual_obs.objects if obj["name"] in speech_lower), None)\n            if visible_obj:\n                return f"Yes, I can see the {visible_obj[\'name\']} in front of me. It looks like it\'s positioned in the center of my view."\n\n        # Check if user is asking to do something with visible objects\n        if "pick up" in speech_lower or "grab" in speech_lower:\n            for obj in visual_obs.objects:\n                if obj["name"] in speech_lower:\n                    return f"I can see the {obj[\'name\']} and I can pick it up for you. Reaching out to grasp it now."\n\n        # Default responses\n        if "hello" in speech_lower or "hi" in speech_lower:\n            return "Hello! I can see several objects in front of me including a cup, book, and phone. How can I help you?"\n\n        if "what do you see" in speech_lower or "what\'s there" in speech_lower:\n            if visual_obs.objects:\n                obj_names = [obj["name"] for obj in visual_obs.objects]\n                obj_str = ", ".join(obj_names)\n                return f"I see a {obj_str} in front of me right now."\n            else:\n                return "I don\'t see any specific objects in front of me right now."\n\n        return f"I heard you say \'{speech_input}\'. I can see objects in front of me, including {len(visual_obs.objects)} that I can identify."\n\n    def speak_response(self, response: str):\n        """Speak the response using TTS"""\n        print(f"Robot says: {response}")\n        self.tts_engine.say(response)\n        self.tts_engine.runAndWait()\n\n    def run_dialogue_cycle(self):\n        """Run a single cycle of multimodal dialogue"""\n        # Capture visual observation\n        visual_obs = self.capture_visual_observations()\n\n        if visual_obs:\n            self.conversation_context["last_seen_objects"] = visual_obs.objects\n            print(f"Objects detected: {[obj[\'name\'] for obj in visual_obs.objects]}")\n\n        # Get speech input (with timeout)\n        try:\n            speech_event = self.speech_queue.get(timeout=5)\n            speech_input = speech_event["text"]\n\n            # Process multimodal input\n            response = self.process_multimodal_input(speech_input, visual_obs)\n\n            # Speak response\n            self.speak_response(response)\n\n            return True\n\n        except queue.Empty:\n            print("No speech input received within timeout")\n            return False\n\n    def start_continuous_dialogue(self):\n        """Start continuous multimodal dialogue"""\n        print("Starting continuous multimodal dialogue...")\n        print("Speak to the robot, and it will respond based on what it sees and hears.")\n        print("Press Ctrl+C to stop.")\n\n        # Start listening\n        self.start_listening_async()\n\n        try:\n            while True:\n                success = self.run_dialogue_cycle()\n                if not success:\n                    time.sleep(0.1)  # Small pause to prevent busy waiting\n        except KeyboardInterrupt:\n            print("\\nStopping multimodal dialogue system...")\n\n# Example usage\ndef main():\n    # This would normally be your OpenAI API key\n    gpt_api_key = "fake-api-key-for-demo"  # Placeholder for demonstration\n\n    try:\n        # Initialize multimodal dialogue system\n        print("Initializing Multimodal Dialogue System...")\n        dialogue_system = MultimodalDialogueSystem(gpt_api_key)\n\n        # Run a few demonstration cycles\n        print("Running demonstration of multimodal dialogue...")\n\n        # Simulate visual observations\n        demo_objects = [\n            {"name": "cup", "confidence": 0.89, "bbox": [100, 150, 150, 200]},\n            {"name": "book", "confidence": 0.76, "bbox": [200, 100, 300, 250]},\n            {"name": "phone", "confidence": 0.92, "bbox": [350, 200, 400, 300]}\n        ]\n\n        visual_obs = VisualObservation(\n            objects=demo_objects,\n            scene_description="I can see a cup, book, and phone on a table in front of me.",\n            timestamp=time.time()\n        )\n\n        # Simulate different user inputs\n        demo_inputs = [\n            "Hello robot",\n            "What do you see?",\n            "Can you pick up the phone?",\n            "Is there a cup there?",\n            "Grab the cup"\n        ]\n\n        for user_input in demo_inputs:\n            print(f"\\nUser says: \'{user_input}\'")\n\n            # Process multimodal input\n            response = dialogue_system.process_multimodal_input(user_input, visual_obs)\n\n            # Display response\n            print(f"Robot responds: \'{response}\'")\n\n        print("\\nDemonstration complete!")\n\n    except Exception as e:\n        print(f"Error running multimodal dialogue: {e}")\n\nif __name__ == "__main__":\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Expected Output:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Initializing Multimodal Dialogue System...\nRunning demonstration of multimodal dialogue...\n\nUser says: 'Hello robot'\nRobot responds: 'Hello! I can see several objects in front of me including a cup, book, and phone. How can I help you?'\n\nUser says: 'What do you see?'\nRobot responds: 'I see a cup, book, phone in front of me right now.'\n\nUser says: 'Can you pick up the phone?'\nRobot responds: 'I can see the phone and I can pick it up for you. Reaching out to grasp it now.'\n\nUser says: 'Is there a cup there?'\nRobot responds: 'Yes, I can see the cup in front of me. It looks like it's positioned in the center of my view.'\n\nUser says: 'Grab the cup'\nRobot responds: 'I can see the cup and I can pick it up for you. Reaching out to grasp it now.'\n\nDemonstration complete!\n"})}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-on Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate a real speech recognition API (like Google Cloud Speech-to-Text) with your robot system"}),"\n",(0,i.jsx)(n.li,{children:"Implement a dialogue manager that maintains conversation state and handles follow-up questions"}),"\n",(0,i.jsx)(n.li,{children:"Create a multimodal interface that combines visual object recognition with natural language commands"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"Conversational robotics represents the convergence of natural language processing, computer vision, and robotic control to create more intuitive human-robot interfaces. By integrating large language models like GPT with robotic systems, robots can understand and respond to natural language commands while considering visual context. Successful implementation requires careful integration of speech recognition, natural language understanding, and multimodal perception systems to create responsive and contextually aware robot assistants."}),"\n",(0,i.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,i.jsx)(n.p,{children:"This chapter requires specialized hardware and software:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Microphone array for speech recognition"}),"\n",(0,i.jsx)(n.li,{children:"Camera for visual perception"}),"\n",(0,i.jsx)(n.li,{children:"Speaker for text-to-speech output"}),"\n",(0,i.jsx)(n.li,{children:"Internet connection for cloud-based language models"}),"\n",(0,i.jsx)(n.li,{children:"Real-time computing platform for responsive interaction"}),"\n",(0,i.jsx)(n.li,{children:"Robot platform with manipulation capabilities (optional for basic exercises)"}),"\n",(0,i.jsx)(n.li,{children:"Privacy-compliant data handling setup for speech data"}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var t=o(6540);const i={},s=t.createContext(i);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);